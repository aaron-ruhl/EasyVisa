{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaron-ruhl/EasyVisa/blob/main/DSBA_Project_ET_EasyVisa_Fullcode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT5OogJVFbwu"
      },
      "source": [
        "# EasyVisa Project\n",
        "\n",
        "## Context:\n",
        "\n",
        "Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\n",
        "\n",
        "The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\n",
        "\n",
        "OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment.\n",
        "\n",
        "## Objective:\n",
        "\n",
        "In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\n",
        "\n",
        "The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired your firm EasyVisa for data-driven solutions. You as a data scientist have to analyze the data provided and, with the help of a classification model:\n",
        "\n",
        "* Facilitate the process of visa approvals.\n",
        "* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status.\n",
        "\n",
        "\n",
        "## Data Description\n",
        "\n",
        "The data contains the different attributes of the employee and the employer. The detailed data dictionary is given below.\n",
        "\n",
        "* case_id: ID of each visa application\n",
        "* continent: Information of continent the employee\n",
        "* education_of_employee: Information of education of the employee\n",
        "* has_job_experience: Does the employee has any job experience? Y= Yes; N = No\n",
        "* requires_job_training: Does the employee require any job training? Y = Yes; N = No\n",
        "* no_of_employees: Number of employees in the employer's company\n",
        "* yr_of_estab: Year in which the employer's company was established\n",
        "* region_of_employment: Information of foreign worker's intended region of employment in the US.\n",
        "* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.\n",
        "* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\n",
        "* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position\n",
        "* case_status:  Flag indicating if the Visa was certified or denied"
      ],
      "id": "AT5OogJVFbwu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dirty-island"
      },
      "source": [
        "## Importing necessary libraries and data"
      ],
      "id": "dirty-island"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "statewide-still"
      },
      "outputs": [],
      "source": [
        "# Reading and manipulating dataframes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualizations of data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "id": "statewide-still"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5gtJkSaA16m"
      },
      "outputs": [],
      "source": [
        "# Using Google Colab, I first uploaded the file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "id": "Q5gtJkSaA16m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX1eh2iSA7Wq"
      },
      "outputs": [],
      "source": [
        "# Then used pandas to read the file with the help of 'io'\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['EasyVisa.csv']))"
      ],
      "id": "vX1eh2iSA7Wq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "desperate-infection"
      },
      "source": [
        "## Data Overview\n",
        "\n",
        "- Observations\n",
        "- Sanity checks"
      ],
      "id": "desperate-infection"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets get a quick look at the data**"
      ],
      "metadata": {
        "id": "oIPD7BcUpROF"
      },
      "id": "oIPD7BcUpROF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e44be16d"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ],
      "id": "e44be16d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WdsT2n0b8Zv"
      },
      "outputs": [],
      "source": [
        "data.tail()"
      ],
      "id": "-WdsT2n0b8Zv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the shape of the data?**"
      ],
      "metadata": {
        "id": "t4IbucF6prnL"
      },
      "id": "t4IbucF6prnL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a58a1e75"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ],
      "id": "a58a1e75"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 25480 observations, with only 12 columns. Could be result of trimming off excessive features. Either way it is a small amount which I think will help with the time required to build models."
      ],
      "metadata": {
        "id": "BsLpdNSLp3dj"
      },
      "id": "BsLpdNSLp3dj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What about dtypes and non-null counts?**"
      ],
      "metadata": {
        "id": "zxg1zuC6pvmM"
      },
      "id": "zxg1zuC6pvmM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a4a1dad"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ],
      "id": "6a4a1dad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mostly categorical, with a few numerical columns\n",
        "- Non-null counts are all clean"
      ],
      "metadata": {
        "id": "VqvH_tKFqhnZ"
      },
      "id": "VqvH_tKFqhnZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of numerical columns**"
      ],
      "metadata": {
        "id": "qxUErcS7yY8a"
      },
      "id": "qxUErcS7yY8a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aaabe8c"
      },
      "outputs": [],
      "source": [
        "data.describe(include=np.number).T"
      ],
      "id": "4aaabe8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDcapXnIcRb-"
      },
      "source": [
        "`no_of_employees`\n",
        "  - Negative values like the minimum(-26) should be investigated further.\n",
        "  - Most of the values centered around 2100\n",
        "  - Massive rightward skew, mean of 5667 with a std of 22877.\n",
        "\n",
        "`yr_of_estab`\n",
        "  - Centered around 1990-2000, but the left tail was highly skewed.\n",
        "  - Mean was around 1976 with a std of 42.\n",
        "  - Older businesses have a harder time getting approved?\n",
        "\n",
        "`prevailing_wage`\n",
        "  - Immense range of values from 2.14 to 319210.27\n",
        "  - Most of the values were centered around 70000\n",
        "  - Mean of 74455.8 and std of 52816"
      ],
      "id": "HDcapXnIcRb-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of categorical columns(plus binary)**"
      ],
      "metadata": {
        "id": "wmbt0CH8ymaR"
      },
      "id": "wmbt0CH8ymaR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8029ebf"
      },
      "outputs": [],
      "source": [
        "data.describe(include=\"object\").T"
      ],
      "id": "e8029ebf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f60f875"
      },
      "source": [
        "Observations:\n",
        "\n",
        "`Case_id`:\n",
        "  - This is just a unique variable that could be dropped for this situation.\n",
        "\n",
        "`Continent`:\n",
        "  - Mostly \"Asia\" with around eight thousand spread across the other 5 categories.\n",
        "\n",
        "`Education_of_employee`:\n",
        "  - Might work as an ordinal variable, with education increasing at each of the 4 levels.\n",
        "    - Basically, they stack because to get a masters one must first get the first two. so technically it is the \"third\" level.\n",
        "    - Also reducing the overall amount of features that I feed into the model building process seems like a prudent choice and I have seen this particular category done this way before.\n",
        "\n",
        "`Has_job_experience`:\n",
        "  - Seems it was around 50/50 \"Y\" or \"N\".\n",
        "\n",
        "`Requires_job_training`:\n",
        "  - Most of the responses were \"N\", only around 3000 \"Y\" out of 25480.\n",
        "\n",
        "`Region_of_employment`:\n",
        "- Does not seem like any of the classes dominated this feature.\n",
        "- Five different regions.\n",
        "\n",
        "`Unit_of_wage`:\n",
        "  - Year was a highly frequent occurence and might overpower the minority classes in this feature.\n",
        "\n",
        "`Full_time_position`:\n",
        "  - Skewed with only 2 classes like 'requires_job_training', with \"Y\" as the most frequent.\n",
        "\n",
        "`Case_status`:\n",
        "  - Split 1/3 \"Denied\" and 2/3 \"Certified\".\n",
        "    - \"Denied\" is the minority class.\n",
        "\n",
        "OVERALL:\n",
        "  - Exceptionally low cardinality in all of these variables. Rare classes will be my primary concern."
      ],
      "id": "1f60f875"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick isnull() check**"
      ],
      "metadata": {
        "id": "Ib6K9WyQy18T"
      },
      "id": "Ib6K9WyQy18T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19d7df74"
      },
      "outputs": [],
      "source": [
        "data.isnull().mean()"
      ],
      "id": "19d7df74"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick duplicate checks**"
      ],
      "metadata": {
        "id": "ltxsj0El3009"
      },
      "id": "ltxsj0El3009"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05d2e862"
      },
      "outputs": [],
      "source": [
        "len(data.case_id.unique())"
      ],
      "id": "05d2e862"
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "I23zagD6yHSD"
      },
      "id": "I23zagD6yHSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seasonal-calibration"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "id": "seasonal-calibration"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e67ac961"
      },
      "outputs": [],
      "source": [
        "# This function just works very well and makes great looking graphs so I decided to use it again.\n",
        "\n",
        "\n",
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(\"-\" * 120)\n",
        "    print(tab1)\n",
        "\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\",\n",
        "        frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ],
      "id": "e67ac961"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classified-traveler"
      },
      "source": [
        "**Leading Questions**:\n"
      ],
      "id": "classified-traveler"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Those with higher education may want to travel abroad for a well-paid job. Does education play a role in Visa certification?\n",
        "\n"
      ],
      "metadata": {
        "id": "6IfPQy_cIUru"
      },
      "id": "6IfPQy_cIUru"
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(data,'education_of_employee','case_status')"
      ],
      "metadata": {
        "id": "avUHuwzAIi-k"
      },
      "id": "avUHuwzAIi-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes, it appears to be important according to this data. Appears more likely to get a visa if one has a higher level of education."
      ],
      "metadata": {
        "id": "vY_PkrAUI9uY"
      },
      "id": "vY_PkrAUI9uY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does the visa status vary across different continents?\n"
      ],
      "metadata": {
        "id": "deHgzO6oIUzQ"
      },
      "id": "deHgzO6oIUzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(data,'continent','case_status')"
      ],
      "metadata": {
        "id": "gCIGU37bJP3a"
      },
      "id": "gCIGU37bJP3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Changes across different continents but the range is overall pretty small."
      ],
      "metadata": {
        "id": "SVbATzYUJkj3"
      },
      "id": "SVbATzYUJkj3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Experienced professionals might look abroad for opportunities to improve their lifestyles and career development. Does work experience influence visa status?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ek--hpCCIU7V"
      },
      "id": "Ek--hpCCIU7V"
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(data,'has_job_experience','case_status')"
      ],
      "metadata": {
        "id": "2V4GVDmXJvB-"
      },
      "id": "2V4GVDmXJvB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Appears to indicate that applicants with work experience were accepted more often on average."
      ],
      "metadata": {
        "id": "neNptpklJ44I"
      },
      "id": "neNptpklJ44I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In the United States, employees are paid at different intervals. Which pay unit is most likely to be certified for a visa?\n",
        ""
      ],
      "metadata": {
        "id": "vQcPPS3QIVCi"
      },
      "id": "vQcPPS3QIVCi"
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(data,'unit_of_wage','case_status')"
      ],
      "metadata": {
        "id": "9ghx3VR9KAJY"
      },
      "id": "9ghx3VR9KAJY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yearly is the most likely, but also the vast majority of the cases."
      ],
      "metadata": {
        "id": "68hmOq1RKIGi"
      },
      "id": "68hmOq1RKIGi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. The US government has established a prevailing wage to protect local talent and foreign workers. How does the visa status change with the prevailing wage?"
      ],
      "metadata": {
        "id": "hqhybHfhIVKA"
      },
      "id": "hqhybHfhIVKA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets plot the distribution using matplotlib since it is continuous\n",
        "fig = plt.figure(figsize=(12,9))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "data[data['case_status']==\"Denied\"]['prevailing_wage'].hist(bins=50, ax=ax, color='red')\n",
        "data[data['case_status']==\"Certified\"]['prevailing_wage'].hist(bins=50, ax=ax, color='green', alpha=0.5)\n",
        "\n",
        "ax.set_title('prevailing_wage')\n",
        "ax.set_xlabel('prevailing_wage')\n",
        "ax.legend(['Denied','Certified'], loc='best')"
      ],
      "metadata": {
        "id": "CCRl_7zIKUg_"
      },
      "id": "CCRl_7zIKUg_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Not any major changes except for the lower end of the variable, I explore this further in EDA below."
      ],
      "metadata": {
        "id": "t1mU4sWV5UIZ"
      },
      "id": "t1mU4sWV5UIZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**before splitting, lets get a further look at the data and try to see what else is going on**"
      ],
      "metadata": {
        "id": "ny51h10o4YmF"
      },
      "id": "ny51h10o4YmF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df33ac2b"
      },
      "source": [
        "### Categorical Univariate Analysis"
      ],
      "id": "df33ac2b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`education_of_employee`**"
      ],
      "metadata": {
        "id": "KSTKF-ZK4PvC"
      },
      "id": "KSTKF-ZK4PvC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b2effa3"
      },
      "outputs": [],
      "source": [
        "data.education_of_employee.unique()"
      ],
      "id": "8b2effa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b91eeef9"
      },
      "source": [
        "Education of employee recorded in this dataset could be considered ordinal.\n",
        "<br></br>For example:\n",
        "<br>  'education_of_employee'= {[\"High School\": 1, \"Bachelor's\":2, \"Masters\":3, \"Doctorate\":4]} </br>"
      ],
      "id": "b91eeef9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40b80364"
      },
      "outputs": [],
      "source": [
        "# Let's make a histogram to get familiar with the\n",
        "# variable distribution.\n",
        "\n",
        "fig = data['education_of_employee'].hist(bins=60)\n",
        "\n",
        "fig.set_title('Education of Employee')\n",
        "fig.set_xlabel('Unique Levels of Education')\n",
        "fig.set_ylabel('Number of Applicants')"
      ],
      "id": "40b80364"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d7da506"
      },
      "source": [
        "Mostly within the mid-range: \"Bachelor's\" <--> \"Master's\""
      ],
      "id": "8d7da506"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b899f40"
      },
      "outputs": [],
      "source": [
        "data.education_of_employee.value_counts(normalize=True)"
      ],
      "id": "4b899f40"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcbc067"
      },
      "source": [
        "\"Doctorate\" is a rare class, but it is almost 9% of data. Not exceedingly rare."
      ],
      "id": "cfcbc067"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`continent`**"
      ],
      "metadata": {
        "id": "XpFdMlBY4v2L"
      },
      "id": "XpFdMlBY4v2L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01e76df0"
      },
      "outputs": [],
      "source": [
        "# Let's see what unique categories we have.\n",
        "data.continent.unique()"
      ],
      "id": "01e76df0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc6a6aa0"
      },
      "source": [
        "Decent diversity in this feature."
      ],
      "id": "cc6a6aa0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88333940",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Let's make a histogram to get familiar with the variable distribution.\n",
        "\n",
        "fig = data['continent'].hist(bins=40)\n",
        "\n",
        "fig.set_title('Continent')\n",
        "fig.set_xlabel('Origin-Continent of Applicants')\n",
        "fig.set_ylabel('No. of Applicants')"
      ],
      "id": "88333940"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11911426"
      },
      "source": [
        "As the code below confirms, \"South America\", \"Africa\", and \"Oceania\" were substantially more rare than \"Asia\".\n",
        "<br>\"North America\" and \"Europe\" were more common, but still much less prevalent in the data than \"Asia\".</br>         "
      ],
      "id": "11911426"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c91ced53",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Trying to determine the prevalence of each category\n",
        "data.continent.value_counts(normalize=True)"
      ],
      "id": "c91ced53"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`region_of_employment`**"
      ],
      "metadata": {
        "id": "dGCw1Zcl41d0"
      },
      "id": "dGCw1Zcl41d0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ca59cc4",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Let's make a histogram to get familiar with the variable distribution.\n",
        "\n",
        "fig = data.region_of_employment.hist(bins=50)\n",
        "\n",
        "fig.set_title(\"Region Of Employement\")\n",
        "fig.set_xlabel(\"Regions that these Applicants Prefered\")\n",
        "fig.set_ylabel(\"No. of Applicants\")"
      ],
      "id": "7ca59cc4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c075b9f7"
      },
      "outputs": [],
      "source": [
        "# Trying to determine the prevalence of each category\n",
        "data.region_of_employment.value_counts(normalize=True)"
      ],
      "id": "c075b9f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa630287"
      },
      "source": [
        "\"Island\" is a rare minority of the cases."
      ],
      "id": "aa630287"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`unit_of_wage`**"
      ],
      "metadata": {
        "id": "5xaLleyW49Lh"
      },
      "id": "5xaLleyW49Lh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12b6e670",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Let's make a histogram to get familiar with the variable distribution.\n",
        "\n",
        "fig = data.unit_of_wage.hist(bins=50)\n",
        "\n",
        "fig.set_title(\"\")\n",
        "fig.set_xlabel(\"\")\n",
        "fig.set_ylabel(\"No. of Applicants\")"
      ],
      "id": "12b6e670"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1555e12"
      },
      "outputs": [],
      "source": [
        "# Trying to determine the prevalence of each category\n",
        "data.unit_of_wage.value_counts(normalize=True)"
      ],
      "id": "c1555e12"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdc18d03"
      },
      "source": [
        "Year is the overwhelming majority of cases."
      ],
      "id": "cdc18d03"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Okay! Now lets look at the numerical columns**"
      ],
      "metadata": {
        "id": "4hXt5VXy5D1b"
      },
      "id": "4hXt5VXy5D1b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d6ae8be"
      },
      "source": [
        "### Numerical Univariate Analysis"
      ],
      "id": "8d6ae8be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50da9e37"
      },
      "outputs": [],
      "source": [
        "# I decided to try and do things differently this time (transform) and found this function elsewhere and thought it worked really well for this purpose\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Function to create a histogram, a Q-Q plot and\n",
        "# a boxplot.\n",
        "\n",
        "\n",
        "def diagnostic_plots(df, variable, bins=28):\n",
        "    # The function takes a dataframe (df) and\n",
        "    # the variable of interest as arguments.\n",
        "\n",
        "    # Define figure size.\n",
        "    plt.figure(figsize=(16, 4))\n",
        "\n",
        "    # histogram\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.histplot(df[variable], bins=bins)\n",
        "    plt.title('Histogram')\n",
        "\n",
        "    # Q-Q plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
        "    plt.ylabel('Case Status')\n",
        "\n",
        "    # boxplot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    sns.boxplot(y=df[variable])\n",
        "    plt.title('Boxplot')\n",
        "\n",
        "    plt.show()"
      ],
      "id": "50da9e37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4afc615f",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# make a list of the numerical columns and iterate through that list to create a quick and informative display\n",
        "\n",
        "num_cols = ['no_of_employees','yr_of_estab','prevailing_wage']\n",
        "\n",
        "for specific_col in num_cols:\n",
        "    print('\\n')\n",
        "    diagnostic_plots(data, specific_col)\n",
        "    print('\\n')"
      ],
      "id": "4afc615f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`no_of_employees`\n",
        "  - As mentioned above, mostly centered around 2000. Unfortunately this means that a massive amount of data was an outlier.\n",
        "  - Massive range could cause issues with model building time.\n",
        "\n",
        "`yr_of_estab`\n",
        "  - Some sort of grouping might be appropriate here, but I have not been properly introduced to such a technique.\n",
        "    - It certainly will not prevent me from finishing the requirements of this project in its current form. I focused on transforming the other two continuous variables.\n",
        "\n",
        "`prevailing_wage`\n",
        "  - Not bad, but there was a large amount of outliers and the lower end seems very skewed. Basically looks like a bimodal distribution.\n",
        "  - Massive range could cause issues with model building time."
      ],
      "metadata": {
        "id": "AFhM-up_7Mtu"
      },
      "id": "AFhM-up_7Mtu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's Check out those negative values for 'no_of_employees'**"
      ],
      "metadata": {
        "id": "g2F1TWDgC9Ys"
      },
      "id": "g2F1TWDgC9Ys"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDaUJ872W1K2"
      },
      "outputs": [],
      "source": [
        "print(len(data[data['no_of_employees']<=0]),\"rows with less than zero employees\")\n",
        "#data[data['no_of_employees']<=0]"
      ],
      "id": "sDaUJ872W1K2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ultimate solution could be imputing these values and adding a missing indicator for each of these rows because they do appear to be pretty normal otherwise. This would technically be adding an additional column for only 30 observations! They better be significant!\n",
        "\n",
        "- Anyways I decided, because we have not learned that yet and I still have nightmares from the hackathon, to just remove these rows for now.\n",
        "\n",
        "- Also, one could say that removing 33 potentially erroneous observations is not very likely to add significant bias to predictions made using a set of 25480 observations. Not impossible, but very unlikely."
      ],
      "metadata": {
        "id": "EO2S6I2fAlj0"
      },
      "id": "EO2S6I2fAlj0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "365e07e5"
      },
      "source": [
        "### Bivariate Analysis"
      ],
      "id": "365e07e5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, I display all of the categorical/binary variables 'case_status', split into different classes of each feature.**"
      ],
      "metadata": {
        "id": "gDagJgauEI_7"
      },
      "id": "gDagJgauEI_7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "523c4254",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Making a list that I call 'cat_cols'\n",
        "cat_cols = ['education_of_employee','continent','region_of_employment','unit_of_wage','has_job_experience','requires_job_training','full_time_position']\n",
        "\n",
        "# Initializing total_applicants as the length of the data (25480)\n",
        "total_applicants = len(data)\n",
        "\n",
        "# Iterating through cat_cols, running stacked_barplots and a custom graph to show category frequency!\n",
        "for col in cat_cols:\n",
        "\n",
        "    # Defined above\n",
        "    stacked_barplot(data, col, 'case_status')\n",
        "\n",
        "    # Let's plot the category frequency.\n",
        "    # That is, the percentage of applicants with each label.\n",
        "\n",
        "    temp_df = pd.Series(data[col].value_counts() / total_applicants)\n",
        "\n",
        "    # Make plot with these percentages.\n",
        "    fig = temp_df.sort_values(ascending=False).plot.bar()\n",
        "    fig.set_xlabel(col)\n",
        "\n",
        "    # Add a line at 5 % to flag the threshold for rare categories.\n",
        "    fig.axhline(y=0.05, color='red')\n",
        "    fig.set_ylabel('Percentage of Applicants')\n",
        "    plt.show()"
      ],
      "id": "523c4254"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTyPgsMQMJGf"
      },
      "source": [
        "Observations:\n",
        "- `Education_of_employee` adds an excellent amount of predictive value.  \n",
        "  - \"Doctorate\" applicants were denied visa status in around 15% of the cases; \"Highschool\" applicants were denied in approx. 65% of cases.\n",
        "  - Also the amount of data in all categories is above 5% so no rare cases and low cardinality aswell.\n",
        "- `Continent` adds decent predictive value, but the categories will need to be handled properly.\n",
        "  - Asia was the origin continent for the vast majority of cases. The noise in the data is largely dominated by applicants from Asia with a few rare classes.\n",
        "    - There is definitely some predictive value for \"Africa\" even though it is rare, I will try to keep this category seperate in train/test.\n",
        "    - \"Oceania\" & \"South America\" should be combined into an \"Other\" column.\n",
        "      - Rare cases often cause a mismatch between training and test.\n",
        "      - Little difference in proportion of 'case_status' between these two continents.\n",
        "- `region_of_employement` proportion of 'case_status' was not constant across all classes, but not a very large swing in overall proportion of 'case_status'.\n",
        "  - \"Island\" might cause some issues with building train/test, but I want to try and keep it seperated.\n",
        "- `unit_of_wage` appears to offer an excellent predictive value, but I will need to handle the classes carefully.\n",
        "  - \"Week\" & \"Month\" are rare classes in this data. I will combine them to improve model building performance.\n",
        "    - \"Month\" has only 89 entries so it has a very high chance of causing issues building train/test.\n",
        "    -  Exact same proportion of 'case_status' between these two classes. So seperated didnt no really seem to offer alot of value.\n",
        "    - Also the idea of combining monthly and weekly paid applicants works out because \"technically\" they are collectively in-between \"Year\" and \"Hour\".\n"
      ],
      "id": "kTyPgsMQMJGf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excellent, now let's take a look at the numerical columns**"
      ],
      "metadata": {
        "id": "vyr4yv5ZFpKN"
      },
      "id": "vyr4yv5ZFpKN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d97f7dac",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "num_cols=['no_of_employees','yr_of_estab','prevailing_wage']\n",
        "\n",
        "for specific_col in num_cols:\n",
        "    fig = plt.figure(figsize=(12,9))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    data[data['case_status']==\"Denied\"][specific_col].hist(bins=50, ax=ax, color='red')\n",
        "    data[data['case_status']==\"Certified\"][specific_col].hist(bins=50, ax=ax, color='green', alpha=0.5)\n",
        "\n",
        "    ax.set_title(specific_col)\n",
        "    ax.set_xlabel(specific_col)\n",
        "    ax.legend(['Denied','Certified'], loc='best')"
      ],
      "id": "d97f7dac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZXSi0ZzVQgu"
      },
      "source": [
        "`no_of_employees`:\n",
        "  - based on this graph, this variable seems pretty useless, but there could be alot of useful variation within the lower end of the range.\n",
        "\n",
        "`yr_of_estab`:\n",
        "  - Appears to have a relatively constant frequency of 'case_status'. Perhaps if it was discreetly binned it would give more information. I left this out for simplicity. Also as I mentioned it does not seem all that significant overall.\n",
        "\n",
        "`prevailing_wage`:\n",
        "  - Seems like a significant increase in proportion of \"Denied\" 'case_status' was recorded within the lower end of this feature."
      ],
      "id": "AZXSi0ZzVQgu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's look at the tail end of 'prevailing_wage' & 'no_of_employees'**"
      ],
      "metadata": {
        "id": "c7hoKg91HQv-"
      },
      "id": "c7hoKg91HQv-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27a6d24f"
      },
      "outputs": [],
      "source": [
        "# Starting with 'prevailing_wage', looking at the lower end\n",
        "\n",
        "col_temp = 'prevailing_wage'\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "data[data['case_status']==\"Denied\"][col_temp].hist(bins=1000, ax=ax, color='red')\n",
        "data[data['case_status']==\"Certified\"][col_temp].hist(bins=1000, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.set_xlim(0,10000) # What I refer to as the \"lower end\" (0,10000)\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.legend(['Denied','Certified'], loc='best')"
      ],
      "id": "27a6d24f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Rapidly shifts from being mostly \"Denied\" around 1000 and the proportion onwards is mostly constant."
      ],
      "metadata": {
        "id": "bwdH98LCIxJ1"
      },
      "id": "bwdH98LCIxJ1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a69cb8c"
      },
      "outputs": [],
      "source": [
        "col_temp = 'prevailing_wage'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "data[data['case_status']==\"Denied\"][col_temp].hist(bins=1000, ax=ax, color='red')\n",
        "data[data['case_status']==\"Certified\"][col_temp].hist(bins=1000, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.set_xlim(150000,data.prevailing_wage.max())\n",
        "ax.set_ylim(0,25)\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.legend(['Denied','Certified'], loc='best')"
      ],
      "id": "0a69cb8c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Alot of noise in the upper end of this feature."
      ],
      "metadata": {
        "id": "Xh3nzOb8I9-9"
      },
      "id": "Xh3nzOb8I9-9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mechanical-interference"
      },
      "outputs": [],
      "source": [
        "data[data['prevailing_wage']<=900].unit_of_wage.value_counts()"
      ],
      "id": "mechanical-interference"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Almost the entire \"Hourly\" segment was within this range of equal to or less than 900. Okay, perhaps confirming that hourly workers are typically paid less overall."
      ],
      "metadata": {
        "id": "nc3p7OWnJMjE"
      },
      "id": "nc3p7OWnJMjE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now Let's look at 'no_of_employees'**"
      ],
      "metadata": {
        "id": "ffJsu3moLGn6"
      },
      "id": "ffJsu3moLGn6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL12kKhRYMrz"
      },
      "outputs": [],
      "source": [
        "col_temp = 'no_of_employees'\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "data[data['case_status']==\"Denied\"][col_temp].hist(bins=1000, ax=ax, color='red')\n",
        "data[data['case_status']==\"Certified\"][col_temp].hist(bins=1000, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.set_xlim(0,10000)\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.legend(['Denied','Certified'], loc='best')"
      ],
      "id": "gL12kKhRYMrz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Appears to be relatively constant"
      ],
      "metadata": {
        "id": "Kzn7pMfqIu2E"
      },
      "id": "Kzn7pMfqIu2E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aosoZ3SmYpbm"
      },
      "outputs": [],
      "source": [
        "col_temp = 'no_of_employees'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "data[data['case_status']==\"Denied\"][col_temp].hist(bins=1000, ax=ax, color='red')\n",
        "data[data['case_status']==\"Certified\"][col_temp].hist(bins=1000, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.set_xlim(25000,150000)\n",
        "ax.set_ylim(0,25)\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.legend(['Denied','Certified'], loc='best')"
      ],
      "id": "aosoZ3SmYpbm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Also alot of noise in the upper end of this feature."
      ],
      "metadata": {
        "id": "hZCXWYgSL6qo"
      },
      "id": "hZCXWYgSL6qo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQfwmAEaadn1"
      },
      "outputs": [],
      "source": [
        "data[data['no_of_employees']<=7500].region_of_employment.value_counts()"
      ],
      "id": "cQfwmAEaadn1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Includes almost all of the Island category. I guess Islands do not have alot of people. Good to see that the Island category appears normal in this respect because it is so rare within the data."
      ],
      "metadata": {
        "id": "l3FmJrs-IMKT"
      },
      "id": "l3FmJrs-IMKT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's look at the Correlation Matrix**"
      ],
      "metadata": {
        "id": "e8MdbC7wMRD7"
      },
      "id": "e8MdbC7wMRD7"
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(data.corr(),annot=True,linewidths=.5,center=0,cbar=False,cmap=\"Spectral\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6o_e9qfJMQLa"
      },
      "id": "6o_e9qfJMQLa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Not much to say here, besides no correlation detected. Simply just not alot of opportunities for this to occur within this set of features. They seem to be very independent variables."
      ],
      "metadata": {
        "id": "BuAHgf3fMpsf"
      },
      "id": "BuAHgf3fMpsf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Awesome, time to move on to data preprocessing**"
      ],
      "metadata": {
        "id": "UxJz3Q27MMFf"
      },
      "id": "UxJz3Q27MMFf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alleged-spirituality"
      },
      "source": [
        "## Data Preprocessing"
      ],
      "id": "alleged-spirituality"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's convert the objects into categories**"
      ],
      "metadata": {
        "id": "z46Ov_VnNCPZ"
      },
      "id": "z46Ov_VnNCPZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAMLp3SBcz1r"
      },
      "outputs": [],
      "source": [
        "for feature in data.columns: # Loop through all columns in the dataframe\n",
        "    if data[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
        "        data[feature] = pd.Categorical(data[feature])# Replace strings with an integer\n",
        "data.head(10)"
      ],
      "id": "PAMLp3SBcz1r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove the 33 rows mentioned above as I initialize a new df to create/prepare X and y**"
      ],
      "metadata": {
        "id": "msuW7f96QSwc"
      },
      "id": "msuW7f96QSwc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76QTY891LUta"
      },
      "outputs": [],
      "source": [
        "# Initialize df as the entire dataframe minus the 33 rows mentioned above with negative values for 'no_of_employees' I went with CCA for simplicity, it was mentioned in EDA above\n",
        "\n",
        "df = data.drop(data[data.no_of_employees<0].index) #Using the returned indices to drop rows with less than zero employees\n",
        "\n",
        "# Splitting into X and y values seperately so I can keep there original values\n",
        "X = df.drop(['case_status', 'case_id'], axis=1)\n",
        "y = df.pop('case_status')"
      ],
      "id": "76QTY891LUta"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering and Preparing data for Modeling"
      ],
      "metadata": {
        "id": "OkVnOEnwPf-z"
      },
      "id": "OkVnOEnwPf-z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX6CMed0MMMz"
      },
      "outputs": [],
      "source": [
        "# Making a dictionary to perform the replacements I justified above in EDA and other necessary changes such as binary strings\n",
        "\n",
        "replaceStruct = {\n",
        "                \"education_of_employee\": {\"High School\":1,\"Bachelor's\":2,\"Master's\":3,\"Doctorate\":4},\n",
        "                \"unit_of_wage\":     {\"Week\": \"Week_or_Month\", \"Month\": \"Week_or_Month\"},\n",
        "                \"continent\":     {\"South America\": \"Other\", \"Oceania\": \"Other\"},\n",
        "                \"requires_job_training\":     {\"N\": 0, \"Y\": 1 },\n",
        "                \"has_job_experience\":     {\"N\": 0, \"Y\": 1 },\n",
        "                \"full_time_position\":     {\"N\": 0, \"Y\": 1 }\n",
        "                    }\n",
        "\n",
        "# These will be the columns I want to pass into oneHotCols, basically so I am absolutely sure what the function below is doing\n",
        "oneHotCols=[\"unit_of_wage\",\"continent\",\"region_of_employment\"]"
      ],
      "id": "xX6CMed0MMMz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm3uywN8dqAC"
      },
      "outputs": [],
      "source": [
        "# Here I make replacements and initialize it as X_main to prevent altering the original X, save a copy before doing oneHotCols as X_eda, then finally get_dummies for X_main\n",
        "\n",
        "X_main=X.replace(replaceStruct)\n",
        "X_eda = X_main.copy()\n",
        "\n",
        "X_main=pd.get_dummies(X_main, columns=oneHotCols)"
      ],
      "id": "jm3uywN8dqAC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the data into Simple and Transformed sets**"
      ],
      "metadata": {
        "id": "-pPowze8QiJK"
      },
      "id": "-pPowze8QiJK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhBKigOgIlU9"
      },
      "outputs": [],
      "source": [
        "# importing train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Okay I hope this is not a terribly bad way of doing this, but all I am doing is making a \"simple(s)\" and \"transformed(t)\" train_test_split.\n",
        "# Later I also make one for Logistic Regression by using the unaltered X\n",
        "# The third X_train_EDA is simply to make graphs of the replacements I made to see what happened, that is why there is _,_,_ after it. I just do not need those variables.\n",
        "\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_main, y, test_size=.30, random_state=1, stratify=y)\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_main, y, test_size=.30, random_state=1, stratify=y)\n",
        "\n",
        "X_train_EDA, _,_,_= train_test_split(X_eda, y, test_size=.30, random_state=1, stratify=y)\n",
        "\n",
        "# Printing out a sanity check, which helps me as a beginner to see if it was working properly\n",
        "\n",
        "print(\"Perfect match between different train/test sets is: \", all(X_train_s == X_train_t),\n",
        "      \"\\n\\nShape of simple(s) train/test: \\n\",X_train_s.shape, X_test_s.shape,\n",
        "      \"\\n\\nShape of transformed(t) train/test: \\n\", X_train_t.shape, X_test_t.shape,\n",
        "      \"\\n\\nProportion of case_status:\\n\", y_train_s.value_counts(normalize=True),sep='')"
      ],
      "id": "YhBKigOgIlU9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pilsIdMXeKLO"
      },
      "outputs": [],
      "source": [
        "# For EDA purposes I am going to stick back on the 'case_status'. Essentially I just want to make cool graphs with the stacked barplots\n",
        "\n",
        "X_train_EDA = X_train_EDA.join(y_train_s)\n",
        "#X_train_EDA"
      ],
      "id": "pilsIdMXeKLO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjgBkh2ZTr1Z"
      },
      "outputs": [],
      "source": [
        "# Quick sanity check that helps when rerunning multiple times\n",
        "\n",
        "print(len(X_train_s[X_train_s.no_of_employees<0]),\"rows with no_of_employees recorded as less than zero.\")"
      ],
      "id": "XjgBkh2ZTr1Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting the 'case_status' to the desired binary variables**"
      ],
      "metadata": {
        "id": "NQGSx84sQ-aQ"
      },
      "id": "NQGSx84sQ-aQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIakWyBVekwK"
      },
      "outputs": [],
      "source": [
        "y_train_s = y_train_s.replace({'Certified':0,'Denied':1})\n",
        "y_train_t = y_train_t.replace({'Certified':0,'Denied':1})"
      ],
      "id": "LIakWyBVekwK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl-RsJZdlT1W"
      },
      "outputs": [],
      "source": [
        "y_test_s = y_test_s.replace({'Certified':0,'Denied':1})\n",
        "y_test_t = y_test_t.replace({'Certified':0,'Denied':1})"
      ],
      "id": "Gl-RsJZdlT1W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I made the minority class the positive case."
      ],
      "metadata": {
        "id": "GNTtvpcHRQ8o"
      },
      "id": "GNTtvpcHRQ8o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "541498ab"
      },
      "source": [
        "### Treating Outliers"
      ],
      "id": "541498ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's try using a log transform on these two columns**"
      ],
      "metadata": {
        "id": "OavlhpNkR_uD"
      },
      "id": "OavlhpNkR_uD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98a3e7df"
      },
      "outputs": [],
      "source": [
        "# same graph defined above for EDA\n",
        "\n",
        "num_cols = ['no_of_employees','prevailing_wage']\n",
        "\n",
        "for specific_col in num_cols:\n",
        "    print('\\n')\n",
        "    X_train_t[specific_col + '_log'] = np.log(X_train_t[specific_col])   # this is the tricky bit, I am making a new column by using the variable name plus '_log' and then calling it in diagnostic_plots()\n",
        "    diagnostic_plots(X_train_t, specific_col+'_log',bins=28)\n",
        "    print('\\n')"
      ],
      "id": "98a3e7df"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Wow, it looks like no_of_employees cleaned up pretty well with a log transform, but still some outliers.\n",
        "- 'prevailing_wage' still is pretty skewed"
      ],
      "metadata": {
        "id": "OPttjmlFSXI0"
      },
      "id": "OPttjmlFSXI0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's try using a squre root transform**"
      ],
      "metadata": {
        "id": "yBfG5lPgSlAH"
      },
      "id": "yBfG5lPgSlAH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35d9632d"
      },
      "outputs": [],
      "source": [
        "# Same as above, except now the transformation is \"**(1/2)\" or square root\n",
        "\n",
        "num_cols = ['no_of_employees','prevailing_wage']\n",
        "\n",
        "for specific_col in num_cols:\n",
        "    print('\\n')\n",
        "    X_train_t[specific_col + '_sqrt'] = (X_train_t[specific_col])**(1/2)\n",
        "    diagnostic_plots(X_train_t, specific_col+'_sqrt',bins=28)\n",
        "    print('\\n')"
      ],
      "id": "35d9632d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "'prevailing_wage' contains almost no outliers with this transformation."
      ],
      "metadata": {
        "id": "010hesgKSjxo"
      },
      "id": "010hesgKSjxo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Okay, I think I know which transforms to apply to both of these columns.**"
      ],
      "metadata": {
        "id": "ylSfy4IzW54m"
      },
      "id": "ylSfy4IzW54m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHbn4795Wj4Y"
      },
      "outputs": [],
      "source": [
        "# Great, so I will do a log transform on 'no_of_employees'\n",
        "# And a square root transform on 'prevailing_wage'\n",
        "\n",
        "X_train_t.drop(['no_of_employees','no_of_employees_sqrt','prevailing_wage','prevailing_wage_log'],axis=1,inplace=True) # Above I was literally creating these columns so now I selectively remove the ones I no longer need"
      ],
      "id": "nHbn4795Wj4Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not Forget to apply same thing to the X_test!**"
      ],
      "metadata": {
        "id": "oHpS6bstR-qq"
      },
      "id": "oHpS6bstR-qq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1gMgI9RPPep"
      },
      "outputs": [],
      "source": [
        "# making the new columns\n",
        "X_test_t['no_of_employees_log']=np.log(X_test_t['no_of_employees'])\n",
        "X_test_t['prevailing_wage_sqrt']=(X_test_t['prevailing_wage'])**(1/2)\n",
        "\n",
        "# dropping the old columns\n",
        "X_test_t.drop(['prevailing_wage','no_of_employees'],axis=1,inplace=True)"
      ],
      "id": "j1gMgI9RPPep"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Notice that I made the \"simple\" X_train_s earlier because I am new to the concept of transforming variables. I just feel more comfortable with being able to double check...even if it was a pain to code it all."
      ],
      "metadata": {
        "id": "b3HHwFsNTi-F"
      },
      "id": "b3HHwFsNTi-F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "difficult-union"
      },
      "source": [
        "## EDA After Splitting and Modifications\n",
        "\n",
        "- It is a good idea to explore the data once again after manipulating it."
      ],
      "id": "difficult-union"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRTPl_wlujlU"
      },
      "outputs": [],
      "source": [
        "X_train_s.shape, X_test_s.shape"
      ],
      "id": "JRTPl_wlujlU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CquT_Uj5t1Vp"
      },
      "outputs": [],
      "source": [
        "X_train_t.shape, X_test_t.shape"
      ],
      "id": "CquT_Uj5t1Vp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical Comparison"
      ],
      "metadata": {
        "id": "FQWb9h6Ycxzb"
      },
      "id": "FQWb9h6Ycxzb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67lmcV0rEs4j"
      },
      "outputs": [],
      "source": [
        "cat_cols = ['education_of_employee','continent','region_of_employment','unit_of_wage','has_job_experience','requires_job_training','full_time_position']\n",
        "\n",
        "# Let's plot the category frequency.\n",
        "# That is, the percentage of houses with each label.\n",
        "\n",
        "total_applicants = len(X_train_EDA)\n",
        "\n",
        "for specific_col in cat_cols:\n",
        "    print('\\n'+specific_col+':')\n",
        "\n",
        "    stacked_barplot(X_train_EDA, specific_col,'case_status')\n",
        "\n",
        "    temp_df = pd.Series(X_train_EDA[specific_col].value_counts() / total_applicants)\n",
        "\n",
        "    # Make plot with these percentages.\n",
        "    fig = temp_df.sort_values(ascending=False).plot.bar()\n",
        "\n",
        "    # Add a line at 5 % to flag the threshold for rare categories.\n",
        "    fig.axhline(y=0.05, color='red')\n",
        "    fig.set_ylabel('Percentage of Applicants')\n",
        "    plt.show()"
      ],
      "id": "67lmcV0rEs4j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**X_train is capturing similar information as seen in original data even after those replacements**"
      ],
      "metadata": {
        "id": "FuRsWvoLVuUN"
      },
      "id": "FuRsWvoLVuUN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiKKM6SOExLd"
      },
      "source": [
        "### Numerical Comparison"
      ],
      "id": "MiKKM6SOExLd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KD2i4EfD_31"
      },
      "outputs": [],
      "source": [
        "# Let's plot the 'no_of_employees' relationship with 'case_status' in X_train_s\n",
        "\n",
        "col_temp = 'no_of_employees'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Denied\"][col_temp].hist(bins=28, ax=ax, color='red')\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Certified\"][col_temp].hist(bins=28, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_title(col_temp)\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.legend(['X_train_s - Denied','X_train_s - Certified'], loc='best');"
      ],
      "id": "5KD2i4EfD_31"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the 'no_of_employees_log' relationship with 'case_status' as it would be in X_train_t\n",
        "\n",
        "col_temp = 'no_of_employees'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "X_train_EDA[col_temp+'_log'] = np.log(X_train_EDA[col_temp])\n",
        "\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Denied\"][col_temp+'_log'].hist(bins=28, ax=ax, color='red')\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Certified\"][col_temp+'_log'].hist(bins=28, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_title(col_temp+'_log')\n",
        "ax.set_xlabel(col_temp+'_log')\n",
        "ax.legend(['X_train_t - Denied','X_train_t - Certified'], loc='best');"
      ],
      "metadata": {
        "id": "-H1PEf_9YBzJ"
      },
      "id": "-H1PEf_9YBzJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similar to original distribution for X_train_s and the transformation could  be helping for X_train_t. Significant predictive value is not very clear from either graph and it almost looks like a relatively constant proportion of 'case_status'**"
      ],
      "metadata": {
        "id": "avEFK2ukZxmW"
      },
      "id": "avEFK2ukZxmW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoZBWfaTbbjP"
      },
      "outputs": [],
      "source": [
        "# Let's plot the 'prevailing_wage' relationship with 'case_status' in X_train_s\n",
        "\n",
        "col_temp = 'prevailing_wage'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Denied\"][col_temp].hist(bins=50, ax=ax, color='red')\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Certified\"][col_temp].hist(bins=50, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_title(col_temp)\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.legend(['X_train_s - Denied','X_train_s - Certified'], loc='best');"
      ],
      "id": "hoZBWfaTbbjP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the 'prevailing_wage_sqrt' relationship with 'case_status' in X_train_t\n",
        "\n",
        "col_temp = 'prevailing_wage'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "X_train_EDA[col_temp+'_sqrt'] = X_train_EDA[col_temp]**(1/2)\n",
        "\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Denied\"][col_temp+'_sqrt'].hist(bins=50, ax=ax, color='red')\n",
        "X_train_EDA[X_train_EDA['case_status']==\"Certified\"][col_temp+'_sqrt'].hist(bins=50, ax=ax, color='green', alpha=0.8)\n",
        "\n",
        "ax.set_title(col_temp+'_sqrt')\n",
        "ax.set_xlabel(col_temp+'_sqrt')\n",
        "ax.legend(['X_train_t - Denied','X_train_t - Certified'], loc='best');"
      ],
      "metadata": {
        "id": "H68miHKnYBtW"
      },
      "id": "H68miHKnYBtW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Also looks very similar to the original data and the trasformation might help the models \"capture\" all the information within the lower end of this distribution.\n",
        "Appears to have almost three seperate clusters. High, medium, and low. Unfortunately, I was unsure how to use that information.**"
      ],
      "metadata": {
        "id": "xr209llAaHg1"
      },
      "id": "xr209llAaHg1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVkNo_AKEg0k"
      },
      "outputs": [],
      "source": [
        "col_temp = 'yr_of_estab'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "data[col_temp].hist(bins=30, ax=ax, color='yellow')\n",
        "X_train_s[col_temp].hist(bins=30, ax=ax, color='rebeccaPurple', alpha=0.8)\n",
        "\n",
        "ax.set_title(col_temp)\n",
        "ax.set_xlabel(col_temp)\n",
        "ax.legend(['Data','X_train_s'], loc='best');"
      ],
      "id": "lVkNo_AKEg0k"
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(X_train_s.loc[:,['no_of_employees','prevailing_wage','yr_of_estab']].corr(),annot=True,linewidths=.5,center=0,cbar=False,cmap=\"Spectral\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IDml4z8RaXQB"
      },
      "id": "IDml4z8RaXQB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(X_train_t.loc[:,['no_of_employees_log','prevailing_wage_sqrt','yr_of_estab']].corr(),annot=True,linewidths=.5,center=0,cbar=False,cmap=\"Spectral\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9TwCKWCNaddv"
      },
      "id": "9TwCKWCNaddv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not to spend too long here, this all looks great! Now it is time to start building some models**"
      ],
      "metadata": {
        "id": "YNRbhx0ZaU2P"
      },
      "id": "YNRbhx0ZaU2P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "domestic-iceland"
      },
      "source": [
        "## Model Building"
      ],
      "id": "domestic-iceland"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvxT6YbxjwBL"
      },
      "outputs": [],
      "source": [
        "## Function to create confusion matrix that was borrowed from Great Learning because it worked well here\n",
        "def make_confusion_matrix(model,y_actual,X_test,labels=[0, 1]):\n",
        "    '''\n",
        "    model : classifier to predict values of X\n",
        "    y_actual : ground truth\n",
        "\n",
        "    '''\n",
        "    y_predict = model.predict(X_test)\n",
        "\n",
        "    cm=metrics.confusion_matrix(y_actual, y_predict, labels=[0, 1])\n",
        "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
        "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
        "              zip(group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    plt.figure(figsize = (10,7))\n",
        "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "id": "WvxT6YbxjwBL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWzqhu7ajwFY"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision that was also borrowed from Great Learning\n",
        "def get_metrics_score(model, X_train, X_test, y_train, y_test,flag=True):\n",
        "    '''\n",
        "    model : classifier to predict values of X\n",
        "\n",
        "    '''\n",
        "    # defining an empty list to store train and test results\n",
        "    score_list=[]\n",
        "    pred_train = model.predict(X_train)\n",
        "    pred_test = model.predict(X_test)\n",
        "\n",
        "    train_acc = model.score(X_train,y_train)\n",
        "    test_acc = model.score(X_test,y_test)\n",
        "\n",
        "    train_recall = metrics.recall_score(y_train,pred_train)\n",
        "    test_recall = metrics.recall_score(y_test,pred_test)\n",
        "\n",
        "    train_precision = metrics.precision_score(y_train,pred_train)\n",
        "    test_precision = metrics.precision_score(y_test,pred_test)\n",
        "\n",
        "    train_f1 = metrics.f1_score(y_train,pred_train)\n",
        "    test_f1 = metrics.f1_score(y_test,pred_test)\n",
        "\n",
        "    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))\n",
        "\n",
        "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
        "    if flag == True:\n",
        "        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n",
        "        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n",
        "        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n",
        "        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n",
        "        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n",
        "        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n",
        "        print(\"F1 on training set : \",metrics.f1_score(y_train,pred_train))\n",
        "        print(\"F1 on test set : \",metrics.f1_score(y_test,pred_test))\n",
        "\n",
        "    return pd.DataFrame(score_list) # returning the list with train and test scores...as a dataframe to help with comparing models later."
      ],
      "id": "JWzqhu7ajwFY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epphCVQm70G_"
      },
      "source": [
        "### Logistic Regression"
      ],
      "id": "epphCVQm70G_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Building this model for extra practice and doing so helps give me additional context for analysis. Plus it does not take a long time to train this model.\n",
        "- Same goes for the KNN model."
      ],
      "metadata": {
        "id": "UJUSunmasSGo"
      },
      "id": "UJUSunmasSGo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making a seperate train/test for logistic regression(requires a constant column and drop_first=True)**"
      ],
      "metadata": {
        "id": "FFY0BGufrRl2"
      },
      "id": "FFY0BGufrRl2"
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries I will need for LogisticRegression model\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "X_log=X.replace(replaceStruct)\n",
        "X_log=add_constant(X_log)\n",
        "X_log=pd.get_dummies(X_log, columns=oneHotCols,drop_first=True)"
      ],
      "metadata": {
        "id": "07iJQt22lW0S"
      },
      "id": "07iJQt22lW0S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5nVjyp9aTlG"
      },
      "outputs": [],
      "source": [
        "# splitting the data\n",
        "X_train_log,X_test_log,y_train_log,y_test_log=train_test_split(X_log, y, test_size=.30, random_state=1, stratify=y)"
      ],
      "id": "-5nVjyp9aTlG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQWkRyoPh76Y"
      },
      "outputs": [],
      "source": [
        "# making sure we have the right dtype for our target and train\n",
        "y_train_log = y_train_log.replace({'Certified':0,'Denied':1})\n",
        "y_test_log = y_test_log.replace({'Certified':0,'Denied':1})"
      ],
      "id": "dQWkRyoPh76Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDTu16-pfWsq"
      },
      "outputs": [],
      "source": [
        "X_train_log.info()"
      ],
      "id": "QDTu16-pfWsq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Logistic Regression Model**"
      ],
      "metadata": {
        "id": "lxJ4wanYrfMe"
      },
      "id": "lxJ4wanYrfMe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThF9b96y75d5"
      },
      "outputs": [],
      "source": [
        "# fitting the model on training set\n",
        "logit = sm.Logit(y_train_log, X_train_log.astype(float))\n",
        "lg_s = logit.fit()"
      ],
      "id": "ThF9b96y75d5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExBCkKyz8soB"
      },
      "outputs": [],
      "source": [
        "# let's print the logistic regression summary\n",
        "print(lg_s.summary())"
      ],
      "id": "ExBCkKyz8soB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's check the Multicolinearity**"
      ],
      "metadata": {
        "id": "bljzMBE1sMb4"
      },
      "id": "bljzMBE1sMb4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cduEVsGx9nJ7"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# let's check the VIF of the predictors\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train_log.values, i) for i in range(X_train_log.shape[1])],\n",
        "    index=X_train_log.columns,\n",
        "    dtype=float,\n",
        ")\n",
        "print(\"VIF values: \\n\\n{}\\n\".format(vif_series))"
      ],
      "id": "cduEVsGx9nJ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks good, no major problems here. This is expected because linearly increasing variables were not correlated**"
      ],
      "metadata": {
        "id": "6JoOiQGrs42V"
      },
      "id": "6JoOiQGrs42V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping insignificant Columns**"
      ],
      "metadata": {
        "id": "olz02KjKtocO"
      },
      "id": "olz02KjKtocO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKBsI7lqY6gT"
      },
      "outputs": [],
      "source": [
        "# getting accuracy before dropping some insignificant columns\n",
        "\n",
        "pred_train_logModel_s = lg_s.predict(X_train_log) > 0.5\n",
        "pred_train_logModel_s = np.round(pred_train_logModel_s)\n",
        "\n",
        "print(\"Accuracy on training set : \", metrics.accuracy_score(y_train_log, pred_train_logModel_s))"
      ],
      "id": "sKBsI7lqY6gT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPcPDxoyYLly"
      },
      "outputs": [],
      "source": [
        "X_train_log_drop1 = X_train_log.drop(\"yr_of_estab\", axis=1)\n",
        "X_test_log_drop1 = X_test_log.drop(\"yr_of_estab\", axis=1)"
      ],
      "id": "dPcPDxoyYLly"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzGHGS6LYX4T"
      },
      "outputs": [],
      "source": [
        "# fitting the model on training set\n",
        "logit_drop1 = sm.Logit(y_train_log, X_train_log_drop1.astype(float))\n",
        "lg_drop1 = logit_drop1.fit()\n",
        "\n",
        "pred_train_log_drop1 = lg_drop1.predict(X_train_log_drop1)\n",
        "pred_train_log_drop1 = np.round(pred_train_log_drop1)\n",
        "\n",
        "\n",
        "print(\"Accuracy on training set : \", metrics.accuracy_score(y_train_log, pred_train_log_drop1))\n",
        "print(lg_drop1.summary())"
      ],
      "id": "vzGHGS6LYX4T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJeVZDs7ZI_O"
      },
      "outputs": [],
      "source": [
        "X_train_log_drop2 = X_train_log_drop1.drop(\"no_of_employees\", axis=1)\n",
        "X_test_log_drop2 = X_test_log_drop1.drop(\"no_of_employees\", axis=1)"
      ],
      "id": "EJeVZDs7ZI_O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adU_ayfJZhSB"
      },
      "outputs": [],
      "source": [
        "# fitting the model on training set\n",
        "logit_drop2 = sm.Logit(y_train_log, X_train_log_drop2.astype(float))\n",
        "lg_drop2 = logit_drop2.fit()\n",
        "\n",
        "pred_train_log_drop2 = lg_drop2.predict(X_train_log_drop2)\n",
        "pred_train_log_drop2 = np.round(pred_train_log_drop2)\n",
        "\n",
        "\n",
        "print(\"Accuracy on training set : \", metrics.accuracy_score(y_train_log, pred_train_log_drop2))\n",
        "print(lg_drop2.summary())"
      ],
      "id": "adU_ayfJZhSB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YO9Wvz9jHS-"
      },
      "outputs": [],
      "source": [
        "X_train_log_drop3 = X_train_log_drop2.drop(\"prevailing_wage\", axis=1)\n",
        "X_test_log_drop3 = X_test_log_drop2.drop(\"prevailing_wage\", axis=1)"
      ],
      "id": "4YO9Wvz9jHS-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYuRQrGOjHf2"
      },
      "outputs": [],
      "source": [
        "# fitting the model on training set\n",
        "logit_drop3 = sm.Logit(y_train_log, X_train_log_drop3.astype(float))\n",
        "lg_drop3 = logit_drop3.fit()\n",
        "\n",
        "pred_train_log_drop3 = lg_drop3.predict(X_train_log_drop3)\n",
        "pred_train_log_drop3 = np.round(pred_train_log_drop3)\n",
        "\n",
        "\n",
        "print(\"Accuracy on training set : \", metrics.accuracy_score(y_train_log, pred_train_log_drop3))\n",
        "print(lg_drop3.summary())"
      ],
      "id": "kYuRQrGOjHf2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok4Yooe2jk5G"
      },
      "outputs": [],
      "source": [
        "X_train_log_drop4 = X_train_log_drop3.drop(\"region_of_employment_Northeast\", axis=1)\n",
        "X_test_log_drop4 = X_test_log_drop3.drop(\"region_of_employment_Northeast\", axis=1)"
      ],
      "id": "Ok4Yooe2jk5G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT6-emprjlHY"
      },
      "outputs": [],
      "source": [
        "# fitting the model on training set\n",
        "logit_drop4 = sm.Logit(y_train_log, X_train_log_drop4.astype(float))\n",
        "lg_drop4 = logit_drop4.fit()\n",
        "\n",
        "pred_train_log_drop4 = lg_drop4.predict(X_train_log_drop4)\n",
        "pred_train_log_drop4 = np.round(pred_train_log_drop4)\n",
        "\n",
        "\n",
        "print(\"Accuracy on training set : \", metrics.accuracy_score(y_train_log, pred_train_log_drop4))\n",
        "print(lg_drop4.summary())"
      ],
      "id": "QT6-emprjlHY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyeuQAIAj1fZ"
      },
      "outputs": [],
      "source": [
        "X_train_log_drop5 = X_train_log_drop4.drop(\"continent_Asia\", axis=1)\n",
        "X_test_log_drop5 = X_test_log_drop4.drop(\"continent_Asia\", axis=1)"
      ],
      "id": "cyeuQAIAj1fZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1f0PFJYj1jB"
      },
      "outputs": [],
      "source": [
        "# fitting the model on training set\n",
        "logit_drop5 = sm.Logit(y_train_log, X_train_log_drop5.astype(float))\n",
        "lg_drop5 = logit_drop5.fit()\n",
        "\n",
        "pred_train_log_drop5 = lg_drop5.predict(X_train_log_drop5)\n",
        "pred_train_log_drop5 = np.round(pred_train_log_drop5)\n",
        "\n",
        "\n",
        "print(\"Accuracy on training set : \", metrics.accuracy_score(y_train_log, pred_train_log_drop5))\n",
        "print(lg_drop5.summary())"
      ],
      "id": "P1f0PFJYj1jB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g94b-iA7kdfJ"
      },
      "outputs": [],
      "source": [
        "X_train_log_drop6 = X_train_log_drop5.drop(\"continent_North America\", axis=1)\n",
        "X_test_log_drop6 = X_test_log_drop5.drop(\"continent_North America\", axis=1)"
      ],
      "id": "g94b-iA7kdfJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-zSht6PkdlF"
      },
      "outputs": [],
      "source": [
        "# fitting the model on training set\n",
        "logit_drop6 = sm.Logit(y_train_log, X_train_log_drop6.astype(float))\n",
        "lg_drop6 = logit_drop6.fit()\n",
        "\n",
        "pred_train_log_drop6 = lg_drop6.predict(X_train_log_drop6)\n",
        "pred_train_log_drop6 = np.round(pred_train_log_drop6)\n",
        "\n",
        "\n",
        "print(\"Accuracy on training set : \", metrics.accuracy_score(y_train_log, pred_train_log_drop6))\n",
        "print(lg_drop6.summary())"
      ],
      "id": "p-zSht6PkdlF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we can check performance**"
      ],
      "metadata": {
        "id": "wB74pAWGt_zB"
      },
      "id": "wB74pAWGt_zB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYAxXGgYNT8Z"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using statsmodels\n",
        "def model_performance_classification_statsmodels(\n",
        "    model, predictors, target, threshold=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    threshold: threshold for classifying the observation as class 1\n",
        "    \"\"\"\n",
        "\n",
        "    # checking which probabilities are greater than threshold\n",
        "    pred_temp = model.predict(predictors) > threshold\n",
        "    # rounding off the above values to get classes\n",
        "    pred = np.round(pred_temp)\n",
        "\n",
        "    acc = metrics.accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = metrics.recall_score(target, pred)  # to compute Recall\n",
        "    precision = metrics.precision_score(target, pred)  # to compute Precision\n",
        "    f1 = metrics.f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ],
      "id": "xYAxXGgYNT8Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1AedZYhNWZl"
      },
      "outputs": [],
      "source": [
        "log_reg_model_train_perf = model_performance_classification_statsmodels(\n",
        "    lg_drop6, X_train_log_drop6, y_train_log\n",
        ")\n",
        "\n",
        "print(\"Training performance:\")\n",
        "log_reg_model_train_perf"
      ],
      "id": "h1AedZYhNWZl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ-bJodCO0qz"
      },
      "outputs": [],
      "source": [
        "log_reg_model_test_perf = model_performance_classification_statsmodels(\n",
        "    lg_drop6, X_test_log_drop6, y_test_log\n",
        ")\n",
        "\n",
        "print(\"Test performance:\")\n",
        "log_reg_model_test_perf"
      ],
      "id": "NJ-bJodCO0qz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvzSCjGnPoOV"
      },
      "outputs": [],
      "source": [
        "# predicting on training set\n",
        "# threshold is 0.6577, if predicted probability is greater than 0.5 the observation will be classified as 1\n",
        "\n",
        "pred_test_logModel = lg_drop6.predict(X_test_log_drop6) > 0.5\n",
        "pred_test_logModel = np.round(pred_test_logModel)\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test_log, pred_test_logModel)\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
        "              columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "            cm.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                      cm.flatten()/np.sum(cm)]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
        "          zip(group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=labels,fmt='')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "plt.show()"
      ],
      "id": "PvzSCjGnPoOV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Actually seems pretty decent already, but the minority class is being ignored. A theme I discovered throughout this project."
      ],
      "metadata": {
        "id": "op4cAXzJr4HN"
      },
      "id": "op4cAXzJr4HN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbydAAXm70K6"
      },
      "source": [
        "### KNN"
      ],
      "id": "SbydAAXm70K6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4Z6kwwMCo6r"
      },
      "outputs": [],
      "source": [
        "# importing the model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#building the model with the untransformed data\n",
        "kNN_s=KNeighborsClassifier()\n",
        "kNN_s.fit(X_train_s,y_train_s)"
      ],
      "id": "h4Z6kwwMCo6r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO3Cq5gjDPUc"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "kNN_s_score=get_metrics_score(kNN_s,X_train_s,X_test_s, y_train_s, y_test_s) #I alteredd it to take different splits of test data for obvious reasons"
      ],
      "id": "tO3Cq5gjDPUc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqc8YQGzDPdG"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(kNN_s,y_test_s,X_test_s)"
      ],
      "id": "sqc8YQGzDPdG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This model is performing worse than the logistic regression model with a 0.5 threshold, let's try the transformed data."
      ],
      "metadata": {
        "id": "Ew1qYSRbxczc"
      },
      "id": "Ew1qYSRbxczc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9Feccm8DlcP"
      },
      "outputs": [],
      "source": [
        "kNN_t=KNeighborsClassifier()\n",
        "kNN_t.fit(X_train_t,y_train_t)"
      ],
      "id": "i9Feccm8DlcP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrCbHbZNDlfm"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "kNN_t_score=get_metrics_score(kNN_t,X_train_t,X_test_t, y_train_t, y_test_t)"
      ],
      "id": "GrCbHbZNDlfm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dkInnyADljw"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(kNN_t,y_test_t,X_test_t)"
      ],
      "id": "-dkInnyADljw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This model is still not very good, but that was a significant improvement!\n",
        "- Still appears to be overfitting the data."
      ],
      "metadata": {
        "id": "rpBkAQ3eyGjK"
      },
      "id": "rpBkAQ3eyGjK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvOQKILD7xbV"
      },
      "source": [
        "### Decision Tree"
      ],
      "id": "XvOQKILD7xbV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcaXD9y_eGoE"
      },
      "outputs": [],
      "source": [
        "#importing the model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#building the model\n",
        "dtree_s=DecisionTreeClassifier(random_state=1)\n",
        "dtree_s.fit(X_train_s,y_train_s)"
      ],
      "id": "jcaXD9y_eGoE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jw6ZE6roqaP"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "dtree_s_score=get_metrics_score(dtree_s,X_train_s,X_test_s, y_train_s, y_test_s)"
      ],
      "id": "7Jw6ZE6roqaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQiWZeUHp_en"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(dtree_s,y_test_s,X_test_s)"
      ],
      "id": "mQiWZeUHp_en"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Not great, overfitting considerably and the results are poor precision and recall. Let's try the transformed data."
      ],
      "metadata": {
        "id": "M-guIT-_yZmb"
      },
      "id": "M-guIT-_yZmb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ils4-ehBsYul"
      },
      "outputs": [],
      "source": [
        "#building a tree with the transformed data now\n",
        "dtree_t=DecisionTreeClassifier(random_state=1)\n",
        "dtree_t.fit(X_train_t,y_train_t)"
      ],
      "id": "Ils4-ehBsYul"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI3xpVbTu9CD"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "dtree_t_score=get_metrics_score(dtree_t,X_train_t,X_test_t, y_train_t, y_test_t)"
      ],
      "id": "iI3xpVbTu9CD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRGvlRkEsY0O"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(dtree_t,y_test_t,X_test_t)"
      ],
      "id": "NRGvlRkEsY0O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Minor improvement, still poor detection of majority and minority. Let's try it with class weights."
      ],
      "metadata": {
        "id": "7mufSOIjyvyt"
      },
      "id": "7mufSOIjyvyt"
    },
    {
      "cell_type": "code",
      "source": [
        "# building a tree initialized with class weights\n",
        "dtree_t_weighted=DecisionTreeClassifier(random_state=1,class_weight={0:0.3,1:0.7})\n",
        "dtree_t_weighted.fit(X_train_t,y_train_t)"
      ],
      "metadata": {
        "id": "LLDnfhVqFpGq"
      },
      "id": "LLDnfhVqFpGq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "dtree_t_weighted_score=get_metrics_score(dtree_t_weighted,X_train_t,X_test_t, y_train_t, y_test_t)"
      ],
      "metadata": {
        "id": "iJPVcvSDFycG"
      },
      "id": "iJPVcvSDFycG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(dtree_t_weighted,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "ibJ9qwV1FyiY"
      },
      "id": "ibJ9qwV1FyiY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Overall, a minor improvement that I would say is best tree model so far. However all trees seem to be overfitting the data without tuning\n",
        "- I did not attempt post-pruning because this project is already getting pretty long and I think the grader will agree I did enough already!"
      ],
      "metadata": {
        "id": "5TEcOyxSzL7c"
      },
      "id": "5TEcOyxSzL7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUtIcbYwDMg"
      },
      "source": [
        "### **Bagging Models**"
      ],
      "id": "cTUtIcbYwDMg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's try bagging models to improve the performance of Logistic Regression, KNN, and Decision Tree**"
      ],
      "metadata": {
        "id": "7M7Y4rvFz4CO"
      },
      "id": "7M7Y4rvFz4CO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TDRPfKEcWH"
      },
      "source": [
        "#### Bagging Logistic Regression"
      ],
      "id": "G7TDRPfKEcWH"
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#importing the bagging classifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "bagging_lr=BaggingClassifier(base_estimator=LogisticRegression(solver='liblinear',random_state=1),random_state=1)\n",
        "bagging_lr.fit(X_train_log_drop6,y_train_log) #I am using the X_train_drop6 because I previously determined these are the key features and I later realized this might cause issues with stacking, always next time!"
      ],
      "metadata": {
        "id": "r_8RMATwEf3f"
      },
      "id": "r_8RMATwEf3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "bagging_lr_score=get_metrics_score(bagging_lr,X_train_log_drop6,X_test_log_drop6, y_train_log, y_test_log)"
      ],
      "metadata": {
        "id": "cd2MGOuzEwk7"
      },
      "id": "cd2MGOuzEwk7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(bagging_lr,y_test_log,X_test_log_drop6)"
      ],
      "metadata": {
        "id": "rxrwJ5MsEwpD"
      },
      "id": "rxrwJ5MsEwpD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This seems to have offered an improvement in the Logistic Regressions ability to detect approvals or the majority class. Comes at a cost of reduced Recall or detection of the minority class."
      ],
      "metadata": {
        "id": "0Q1aPXEWqx3i"
      },
      "id": "0Q1aPXEWqx3i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bagging KNN"
      ],
      "metadata": {
        "id": "KwpDWe1wF8b8"
      },
      "id": "KwpDWe1wF8b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx3klwhbImr-"
      },
      "outputs": [],
      "source": [
        "#Building the bagging classifier using kNN_s that I defined above.\n",
        "bagging_kNN_s = BaggingClassifier(base_estimator=kNN_s,random_state=1)\n",
        "bagging_kNN_s.fit(X_train_s,y_train_s)"
      ],
      "id": "Qx3klwhbImr-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzfa-60KImwk"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "bagging_kNN_s_score=get_metrics_score(bagging_kNN_s,X_train_s,X_test_s, y_train_s, y_test_s)"
      ],
      "id": "Mzfa-60KImwk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS5fTUYYIm1L"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(bagging_kNN_s,y_test_s,X_test_s)"
      ],
      "id": "TS5fTUYYIm1L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Detecting alot of the majority, but with very poor recall and still overfitting."
      ],
      "metadata": {
        "id": "sQkIsqJl0zLO"
      },
      "id": "sQkIsqJl0zLO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2c35HEoEXRy"
      },
      "outputs": [],
      "source": [
        "#building another bagging classifier but this time the model is fit to X_train_t and y_train_t\n",
        "bagging_kNN_t = BaggingClassifier(base_estimator=kNN_t,random_state=1)\n",
        "bagging_kNN_t.fit(X_train_t,y_train_t)"
      ],
      "id": "x2c35HEoEXRy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q0rKQPZGIXh"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "bagging_kNN_t_score=get_metrics_score(bagging_kNN_t,X_train_t,X_test_t, y_train_t, y_test_t)"
      ],
      "id": "9Q0rKQPZGIXh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqfxmnd5GIaj"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(bagging_kNN_t,y_test_t,X_test_t)"
      ],
      "id": "Cqfxmnd5GIaj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Without the bagging classifier, KNN was significantly improved by using the transformed variables. However by using the bagging classifier the improvement gained from transforming those variables is negligible.\n",
        "- As a side note, KNN was not performing well and takes forever when used with bagging, so I will not use it any further in this project."
      ],
      "metadata": {
        "id": "Eu4gcmVx1tS1"
      },
      "id": "Eu4gcmVx1tS1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK6uX5mDEQGI"
      },
      "source": [
        "#### Bagging Decision Trees"
      ],
      "id": "NK6uX5mDEQGI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVfgRTLfwY0i"
      },
      "outputs": [],
      "source": [
        "#base_estimator for bagging classifier is a decision tree by default\n",
        "bagging_dtree_s=BaggingClassifier(random_state=1)\n",
        "bagging_dtree_s.fit(X_train_s,y_train_s)"
      ],
      "id": "UVfgRTLfwY0i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq-ReQpywg2I"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "bagging_dtree_s_score=get_metrics_score(bagging_dtree_s,X_train_s,X_test_s, y_train_s, y_test_s)"
      ],
      "id": "pq-ReQpywg2I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Jl-4x1wg5h"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(bagging_dtree_s,y_test_s,X_test_s)"
      ],
      "id": "i8Jl-4x1wg5h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Not bad, but still overfitting considerably and poor recall. Let's try using transformed set."
      ],
      "metadata": {
        "id": "32EQvkTw0MBN"
      },
      "id": "32EQvkTw0MBN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIaGhsLVw8_a"
      },
      "outputs": [],
      "source": [
        "bagging_dtree_t = BaggingClassifier(random_state=1)\n",
        "bagging_dtree_t.fit(X_train_t,y_train_t)"
      ],
      "id": "ZIaGhsLVw8_a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntYxZjoSxMM3"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "bagging_dtree_t_score=get_metrics_score(bagging_dtree_t,X_train_t,X_test_t, y_train_t, y_test_t)"
      ],
      "id": "ntYxZjoSxMM3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzBoMxL3xMT0"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(bagging_dtree_t,y_test_t,X_test_t)"
      ],
      "id": "QzBoMxL3xMT0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No significant improvment or decrease in performance. Lets try Random Forest Sampling instead."
      ],
      "metadata": {
        "id": "SFacy01O0dIA"
      },
      "id": "SFacy01O0dIA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIjhWAFsv-JO"
      },
      "source": [
        "#### Random Forest"
      ],
      "id": "pIjhWAFsv-JO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wels85RQxiju"
      },
      "outputs": [],
      "source": [
        "#importing the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#building the model with X_train_s\n",
        "rf_s=RandomForestClassifier(random_state=1)\n",
        "rf_s.fit(X_train_s,y_train_s)"
      ],
      "id": "wels85RQxiju"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U6K_x5bxinB"
      },
      "outputs": [],
      "source": [
        "rf_s_score=get_metrics_score(rf_s,X_train_s,X_test_s,y_train_s,y_test_s)"
      ],
      "id": "3U6K_x5bxinB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YcQA5hzxiqr"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(rf_s,y_test_s,X_test_s)"
      ],
      "id": "6YcQA5hzxiqr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Offers some improvements compared to the bagged decision trees. Still overfitting the data. Let's see what happens if we use the transformed data."
      ],
      "metadata": {
        "id": "dDJLK8BF2mls"
      },
      "id": "dDJLK8BF2mls"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34ylyiauycfX"
      },
      "outputs": [],
      "source": [
        "#building the model with X_train_t\n",
        "rf_t=RandomForestClassifier(random_state=1)\n",
        "rf_t.fit(X_train_t,y_train_t)"
      ],
      "id": "34ylyiauycfX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0tpVc3yyw92"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "rf_t_score=get_metrics_score(rf_t,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "id": "Z0tpVc3yyw92"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxlb45xDyxDP"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(rf_t,y_test_t,X_test_t)"
      ],
      "id": "hxlb45xDyxDP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Again the result of the bagging method is an increased \"generalization\" such that the transformed variables have less impact on the final prediction."
      ],
      "metadata": {
        "id": "9mdTnkCa3EMZ"
      },
      "id": "9mdTnkCa3EMZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9QNi7cuMwRv"
      },
      "source": [
        "### **Boosting Models**"
      ],
      "id": "u9QNi7cuMwRv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjefVaE2wIP7"
      },
      "source": [
        "#### AdaBoost"
      ],
      "id": "pjefVaE2wIP7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7PI83xzypeK"
      },
      "outputs": [],
      "source": [
        "# importing the model\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# Building the model using X_train_s\n",
        "adaBoost_s=AdaBoostClassifier(random_state=1)\n",
        "adaBoost_s.fit(X_train_s,y_train_s)"
      ],
      "id": "o7PI83xzypeK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwEZNgFSyphV"
      },
      "outputs": [],
      "source": [
        "adaBoost_s_score=get_metrics_score(adaBoost_s,X_train_s,X_test_s,y_train_s,y_test_s)"
      ],
      "id": "cwEZNgFSyphV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APRSk_2dypkV"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(adaBoost_s,y_test_s,X_test_s)"
      ],
      "id": "APRSk_2dypkV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Wow this model is doing an excellent job of detecting the majority, but at the cost of reduced detection of the minority or poor recall scores. Does not seem to be overfitting."
      ],
      "metadata": {
        "id": "BjQ7jfo636L7"
      },
      "id": "BjQ7jfo636L7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX5Al_CbsY4m"
      },
      "outputs": [],
      "source": [
        "adaBoost_t = AdaBoostClassifier(random_state=1)\n",
        "adaBoost_t.fit(X_train_t,y_train_t)"
      ],
      "id": "sX5Al_CbsY4m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB7yMuBmvV-m"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "adaBoost_t_score=get_metrics_score(adaBoost_t,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "id": "uB7yMuBmvV-m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0Jef8RvWBh"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(adaBoost_t,y_test_t,X_test_t)"
      ],
      "id": "vc0Jef8RvWBh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The generalization is pretty impressive. No difference at all when building it with the transformed set. In fact all the boosting models showed this behavior and I will avoid repeating it."
      ],
      "metadata": {
        "id": "u_7g-pNn4Igm"
      },
      "id": "u_7g-pNn4Igm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ4uzckhwLgZ"
      },
      "source": [
        "#### Gradient Boost"
      ],
      "id": "rJ4uzckhwLgZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eEVKni20EXF"
      },
      "outputs": [],
      "source": [
        "#importing the model\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "#building the model\n",
        "gradBoost_s=GradientBoostingClassifier(random_state=1)\n",
        "gradBoost_s.fit(X_train_s,y_train_s)"
      ],
      "id": "7eEVKni20EXF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHgeDU3V0EaE"
      },
      "outputs": [],
      "source": [
        "gradBoost_s_score=get_metrics_score(gradBoost_s,X_train_s,X_test_s,y_train_s,y_test_s)"
      ],
      "id": "AHgeDU3V0EaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UubIMox0Edd"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(gradBoost_s,y_test_s,X_test_s)"
      ],
      "id": "3UubIMox0Edd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Similar to performance of adaBoost, but it is \"capturing\" the minority class better with improved recall scores."
      ],
      "metadata": {
        "id": "w2lVv6_J4t3T"
      },
      "id": "w2lVv6_J4t3T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTCJL9H8vmeV"
      },
      "outputs": [],
      "source": [
        "gradBoost_t = GradientBoostingClassifier(random_state=1)\n",
        "gradBoost_t.fit(X_train_t,y_train_t)"
      ],
      "id": "MTCJL9H8vmeV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dyeEJRbvmiH"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "gradBoost_t_score=get_metrics_score(gradBoost_t,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "id": "3dyeEJRbvmiH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLfKdjiUvmk5"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(gradBoost_t,y_test_t,X_test_t)"
      ],
      "id": "CLfKdjiUvmk5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0woV6jBwOxu"
      },
      "source": [
        "#### XGradient Boost"
      ],
      "id": "I0woV6jBwOxu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE0Ny2OI027l"
      },
      "outputs": [],
      "source": [
        "#importing the model\n",
        "from xgboost import XGBClassifier\n",
        "#building the model\n",
        "eXtr_gradBoost_s = XGBClassifier(random_state=1)\n",
        "eXtr_gradBoost_s.fit(X_train_s,y_train_s)"
      ],
      "id": "wE0Ny2OI027l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8xT4rXS02_L"
      },
      "outputs": [],
      "source": [
        "eXtr_gradBoost_s_score=get_metrics_score(eXtr_gradBoost_s,X_train_s,X_test_s,y_train_s,y_test_s)"
      ],
      "id": "P8xT4rXS02_L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEJ5VBIU03DP"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(eXtr_gradBoost_s,y_test_s,X_test_s)"
      ],
      "id": "jEJ5VBIU03DP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Not really much of an improvement. However it did help in some areas without overfitting, which is interesting."
      ],
      "metadata": {
        "id": "HsNi1-Dq5krp"
      },
      "id": "HsNi1-Dq5krp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7fHjUtpv20G"
      },
      "outputs": [],
      "source": [
        "eXtr_gradBoost_t = XGBClassifier(random_state=1)\n",
        "eXtr_gradBoost_t.fit(X_train_t,y_train_t)"
      ],
      "id": "p7fHjUtpv20G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUYsPKUmv24H"
      },
      "outputs": [],
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "eXtr_gradBoost_t_score=get_metrics_score(eXtr_gradBoost_t,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "id": "AUYsPKUmv24H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SltHslhIv27t"
      },
      "outputs": [],
      "source": [
        "make_confusion_matrix(eXtr_gradBoost_t,y_test_t,X_test_t)"
      ],
      "id": "SltHslhIv27t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prime-athletics"
      },
      "source": [
        "##  Will tuning improve the model performance?"
      ],
      "id": "prime-athletics"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From here on I only use the X_train_t or transformed set. In all cases, it performed the same or better than non-transformed X_train_s. Also it would take too long to tune two each of these models!**"
      ],
      "metadata": {
        "id": "_npkg1Ya6TNL"
      },
      "id": "_npkg1Ya6TNL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning Logistic Regression, KNN, & Decision Tree"
      ],
      "metadata": {
        "id": "_j1bWK59vtJP"
      },
      "id": "_j1bWK59vtJP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression Tuning"
      ],
      "metadata": {
        "id": "66pm967ruNrv"
      },
      "id": "66pm967ruNrv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC Curve**"
      ],
      "metadata": {
        "id": "WcWxXk-rv509"
      },
      "id": "WcWxXk-rv509"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsxBaL5CHkVA"
      },
      "outputs": [],
      "source": [
        "# I am essentially using the ROC curve to optimize the threshold by finding the maximum recall without excessively reducing specificity\n",
        "# In other words, I want to detect the minority class\n",
        "\n",
        "logit_roc_auc_train = metrics.roc_auc_score(y_train_log, lg_drop6.predict(X_train_log_drop6))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_train_log, lg_drop6.predict(X_train_log_drop6))\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\n",
        "plt.plot([0, 1], [0, 1], \"r--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver operating characteristic\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "id": "CsxBaL5CHkVA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf0gG-9cKHG9"
      },
      "outputs": [],
      "source": [
        "# Optimal threshold as per AUC-ROC curve\n",
        "# The optimal cut off would be where tpr is high and fpr is low\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_train_log, lg_drop6.predict(X_train_log_drop6))\n",
        "\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold_auc_roc = thresholds[optimal_idx]\n",
        "print(optimal_threshold_auc_roc)"
      ],
      "id": "nf0gG-9cKHG9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we have the optimal threshold from ROC curve, lets check performance**"
      ],
      "metadata": {
        "id": "9XvePGyNICCe"
      },
      "id": "9XvePGyNICCe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3E6jQviNg0J"
      },
      "outputs": [],
      "source": [
        "log_reg_model_train_perf = model_performance_classification_statsmodels(\n",
        "    lg_drop6, X_train_log_drop6, y_train_log,optimal_threshold_auc_roc\n",
        ")\n",
        "\n",
        "print(\"Training performance:\")\n",
        "log_reg_model_train_perf"
      ],
      "id": "B3E6jQviNg0J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XEDcL01OtQW"
      },
      "outputs": [],
      "source": [
        "log_reg_model_test_perf = model_performance_classification_statsmodels(\n",
        "    lg_drop6, X_test_log_drop6, y_test_log,optimal_threshold_auc_roc\n",
        ")\n",
        "\n",
        "print(\"Test performance:\")\n",
        "log_reg_model_test_perf"
      ],
      "id": "9XEDcL01OtQW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Poor accuracy with decent recall"
      ],
      "metadata": {
        "id": "PtHVuXIQIH3p"
      },
      "id": "PtHVuXIQIH3p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPTS5-sOKSLl"
      },
      "outputs": [],
      "source": [
        "# predicting on training set\n",
        "# threshold is 0.3445, if predicted probability is greater than 0.3445 the observation will be classified as 1 or \"Denied\"\n",
        "\n",
        "pred_test_logModel = lg_drop6.predict(X_test_log_drop6) > optimal_threshold_auc_roc\n",
        "pred_test_logModel = np.round(pred_test_logModel)\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test_log, pred_test_logModel)\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
        "              columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "            cm.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                      cm.flatten()/np.sum(cm)]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
        "          zip(group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=labels,fmt='')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "plt.show()"
      ],
      "id": "DPTS5-sOKSLl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This method appears to favor the minority class at the cost of the majority. I will try using the Precision-Recall curve to get a balance between detecting majority and minority class."
      ],
      "metadata": {
        "id": "uO0PSZH3v-IR"
      },
      "id": "uO0PSZH3v-IR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuE4N8NrJUcx"
      },
      "outputs": [],
      "source": [
        "y_scores = lg_drop6.predict(X_train_log_drop6)\n",
        "prec, rec, tre = metrics.precision_recall_curve(y_train_log, y_scores,)\n",
        "\n",
        "\n",
        "def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n",
        "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"recall\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylim([0, 1])\n",
        "    #plt.xlim([0.38,0.4])\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plot_prec_recall_vs_tresh(prec, rec, tre)\n",
        "plt.show()"
      ],
      "id": "NuE4N8NrJUcx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW1owtPcJ-D-"
      },
      "outputs": [],
      "source": [
        "# I find this by zooming in on the graph, this is not perfect, but adequate for this situation\n",
        "optimal_threshold=0.39"
      ],
      "id": "yW1owtPcJ-D-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we have optimal threshold from precision-recall curve**"
      ],
      "metadata": {
        "id": "pAsuT5CqITpJ"
      },
      "id": "pAsuT5CqITpJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0kTUxlbOdQD"
      },
      "outputs": [],
      "source": [
        "log_reg_model_train_perf = model_performance_classification_statsmodels(\n",
        "    lg_drop6, X_train_log_drop6, y_train_log,optimal_threshold\n",
        ")\n",
        "\n",
        "print(\"Training performance:\")\n",
        "log_reg_model_train_perf"
      ],
      "id": "j0kTUxlbOdQD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiDb1ZMDOk0q"
      },
      "outputs": [],
      "source": [
        "log_reg_model_test_perf = model_performance_classification_statsmodels(\n",
        "    lg_drop6, X_test_log_drop6, y_test_log,optimal_threshold\n",
        ")\n",
        "\n",
        "print(\"Test performance:\")\n",
        "log_reg_model_test_perf"
      ],
      "id": "iiDb1ZMDOk0q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Improved accuracy with a loss to recall and gains in precision. However the f1 score is actually lower than before, but not by much."
      ],
      "metadata": {
        "id": "e3vuBnhGIiwV"
      },
      "id": "e3vuBnhGIiwV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaI2lideKBFw"
      },
      "outputs": [],
      "source": [
        "# predicting on training set\n",
        "# threshold is 0.39, if predicted probability is greater than this optimal_threshold the observation will be classified as 1 or \"Denied\"\n",
        "\n",
        "pred_test_logModel = lg_drop6.predict(X_test_log_drop6) > optimal_threshold\n",
        "pred_test_logModel = np.round(pred_test_logModel)\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test_log, pred_test_logModel)\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
        "              columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "            cm.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                      cm.flatten()/np.sum(cm)]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
        "          zip(group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=labels,fmt='')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "\n",
        "plt.show()"
      ],
      "id": "ZaI2lideKBFw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I prefer the second threshold gathered from Precision-Recall curve because it provides better overall accuracy. The goal is to shortlist applicants, so we would like to accurately detect as many approved applicants as possible! This means low false positives and false negatives are equally important.\n",
        "\n",
        "- For now on, I will try to tune models based on f1-score and accuracy because I want to find the best balance between detection of minority and majority class. So that I can shortlist applicants and have minimal incorrectly shortlisted applicants."
      ],
      "metadata": {
        "id": "4-WJYgj8wztF"
      },
      "id": "4-WJYgj8wztF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KNN Tuning"
      ],
      "metadata": {
        "id": "_oVHcOhnv_P4"
      },
      "id": "_oVHcOhnv_P4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing gridsearch for hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "kNN_t_hypertuned=KNeighborsClassifier()\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              \"weights\": ['uniform', 'distance',None],\n",
        "              \"n_neighbors\": np.arange(3,10,1),\n",
        "              \"p\": [1,2],\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(kNN_t_hypertuned, parameters, scoring=acc_scorer, cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_t, y_train_t)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "kNN_t_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "kNN_t_hypertuned.fit(X_train_t, y_train_t)"
      ],
      "metadata": {
        "id": "Q7-TOLsdwXq5"
      },
      "id": "Q7-TOLsdwXq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "kNN_t_hypertuned_score=get_metrics_score(kNN_t_hypertuned,\n",
        "                                         X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "AaUoKdAV3Omu"
      },
      "id": "AaUoKdAV3Omu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(kNN_t_hypertuned,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "6QS0By4A88Kr"
      },
      "id": "6QS0By4A88Kr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model seems to have trouble detecting the minority class. Improved recall compared to original KNN model at the cost of a minor reduction in precision.\n"
      ],
      "metadata": {
        "id": "ol6_X8VO9ksD"
      },
      "id": "ol6_X8VO9ksD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decision Tree Tuning"
      ],
      "metadata": {
        "id": "On779O-uv_Mc"
      },
      "id": "On779O-uv_Mc"
    },
    {
      "cell_type": "code",
      "source": [
        "# To tune different models\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Choose the type of classifier.\n",
        "dtree_t_hypertuned = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              \"class_weight\": [None, \"balanced\"],\n",
        "              \"max_depth\": np.arange(2, 10, 1),\n",
        "              \"max_leaf_nodes\": [10,30,50,80,100],\n",
        "              \"min_samples_split\": [10, 30, 50, 70],\n",
        "              \"min_impurity_decrease\": [0.001, 0.01, 0.1, 0.0]\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(dtree_t_hypertuned, parameters, scoring=acc_scorer, cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_t, y_train_t)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "dtree_t_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "dtree_t_hypertuned.fit(X_train_t, y_train_t)"
      ],
      "metadata": {
        "id": "Ucv_Bp4Cy8Zb"
      },
      "id": "Ucv_Bp4Cy8Zb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "dtree_t_hypertuned_score=get_metrics_score(dtree_t_hypertuned,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "C9DWJQEqz4ir"
      },
      "id": "C9DWJQEqz4ir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(dtree_t_hypertuned,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "7PoRPov-7Ez-"
      },
      "id": "7PoRPov-7Ez-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Not overfitting. Similar performance to the tuned Logistic regression model."
      ],
      "metadata": {
        "id": "_HWioLi06sJs"
      },
      "id": "_HWioLi06sJs"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train_t.columns\n",
        "importances = dtree_t_hypertuned.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E2QcDa6U_KHm"
      },
      "id": "E2QcDa6U_KHm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I focused more on model building in this project, but I do want to mention here that I can now confirm logistic regression and Decision trees both considered `education_of_employee`, `has_job_experience`, `continent_Europe`, and `unit_of_wage` to be significant features for prediction of visa status within this data."
      ],
      "metadata": {
        "id": "azxW3OWPZUmE"
      },
      "id": "azxW3OWPZUmE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning Bagging Models"
      ],
      "metadata": {
        "id": "oHb2k7f3vtyI"
      },
      "id": "oHb2k7f3vtyI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bagging Logistic Regression with tuning"
      ],
      "metadata": {
        "id": "AHAHsAjDKE4A"
      },
      "id": "AHAHsAjDKE4A"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the type of classifier.\n",
        "bagging_lr_hypertuned=BaggingClassifier(base_estimator=LogisticRegression(solver='liblinear',random_state=1),random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              'max_samples': [0.7,0.8,0.9,1],\n",
        "              'max_features': [0.7,0.8,0.9,1],\n",
        "              'n_estimators' : np.arange(80,120,10),\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(bagging_lr_hypertuned, parameters, scoring=acc_scorer,cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_log_drop6, y_train_log)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "bagging_lr_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "bagging_lr_hypertuned.fit(X_train_log_drop6,y_train_log)"
      ],
      "metadata": {
        "id": "TnHBwkLbKN8T"
      },
      "id": "TnHBwkLbKN8T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "bagging_lr_hypertuned_score=get_metrics_score(bagging_lr_hypertuned,X_train_log_drop6,X_test_log_drop6, y_train_log, y_test_log)"
      ],
      "metadata": {
        "id": "ngsaXUi_KpTP"
      },
      "id": "ngsaXUi_KpTP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(bagging_lr_hypertuned,y_test_log,X_test_log_drop6)"
      ],
      "metadata": {
        "id": "LsaqyGosKpbP"
      },
      "id": "LsaqyGosKpbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This model definitely provides an improvement to accuracy, but at the cost of reduced recall. Compared to the base model tuned with a Precision-Recall curve. Detecting 60% of the approved applicants accurately is not bad, perhaps stacking multiple models like this will help."
      ],
      "metadata": {
        "id": "N_AAzdhaOol3"
      },
      "id": "N_AAzdhaOol3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bagging Trees with Tuning"
      ],
      "metadata": {
        "id": "lKP_QcL7wYac"
      },
      "id": "lKP_QcL7wYac"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the type of classifier.\n",
        "bagging_dtree_t_hypertuned = BaggingClassifier(random_state=1,n_jobs=-1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              'max_samples': [0.7,0.8,0.9,1],\n",
        "              'max_features': [0.7,0.8,0.9,1],\n",
        "              'n_estimators' : np.arange(80,120,10),\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "acc_scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(bagging_dtree_t_hypertuned, parameters, scoring=acc_scorer,cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_t, y_train_t)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "bagging_dtree_t_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "bagging_dtree_t_hypertuned.fit(X_train_t, y_train_t)"
      ],
      "metadata": {
        "id": "BWst1Bt270dS"
      },
      "id": "BWst1Bt270dS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using above defined function to get accuracy, recall and precision on train and test set\n",
        "bagging_dtree_t_hypertuned_score=get_metrics_score(bagging_dtree_t_hypertuned,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "HoCpHQhEAjxj"
      },
      "id": "HoCpHQhEAjxj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(bagging_dtree_t_hypertuned,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "1Db2RUBEAl2h"
      },
      "id": "1Db2RUBEAl2h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Okay, so compared to bagging the Logistic Regression models, better detection of the minority, but seems to be overfitting the data. I would rather use the previously tuned decision tree without bagging for stacking. Lets see if Random Forest can reduce the overfitting."
      ],
      "metadata": {
        "id": "G-UTLNxDQHX9"
      },
      "id": "G-UTLNxDQHX9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest with Tuning"
      ],
      "metadata": {
        "id": "Uc9UN28kwZlQ"
      },
      "id": "Uc9UN28kwZlQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the type of classifier.\n",
        "rf_t_hypertuned = RandomForestClassifier(random_state=1,n_jobs=-1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              \"class_weight\":['balanced'],\n",
        "              \"max_depth\": [3,5,7],\n",
        "              \"n_estimators\": np.arange(80,120,10),\n",
        "              \"max_features\": [0.7,0.9,'log2','sqrt',None],\n",
        "              \"max_samples\": [0.7,0.9,None],\n",
        "             }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(rf_t_hypertuned, parameters, scoring=scorer,cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_t, y_train_t)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "rf_t_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "rf_t_hypertuned.fit(X_train_t, y_train_t)"
      ],
      "metadata": {
        "id": "GFCEPFO1VscL"
      },
      "id": "GFCEPFO1VscL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_t_hypertuned_score=get_metrics_score(rf_t_hypertuned,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "w_bNm671uFsM"
      },
      "id": "w_bNm671uFsM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(rf_t_hypertuned,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "rVH6TwPjuFwv"
      },
      "id": "rVH6TwPjuFwv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This model is doing an excellent job of accurately predicting the majority class without excessively ignoring the minority. Also, it is not overfitting like it was with Bagging alone."
      ],
      "metadata": {
        "id": "h5smh-ZRuYcl"
      },
      "id": "h5smh-ZRuYcl"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train_t.columns\n",
        "importances = rf_t_hypertuned.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZgLWk9gi8bDd"
      },
      "id": "ZgLWk9gi8bDd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This model seems to think that prevailing wage was important, which is different. Unfortunately this model seems to favor the minority class. Perhaps this is trying to tell me something I do not yet understand fully."
      ],
      "metadata": {
        "id": "hvrjBFqS8fy9"
      },
      "id": "hvrjBFqS8fy9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning Boosting Models"
      ],
      "metadata": {
        "id": "M6NBMEJ4vuJe"
      },
      "id": "M6NBMEJ4vuJe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBoost Tuning"
      ],
      "metadata": {
        "id": "aL-vvHB0w5Hs"
      },
      "id": "aL-vvHB0w5Hs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the type of classifier.\n",
        "adaBoost_t_hypertuned = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              'n_estimators': np.arange(10,120,10),\n",
        "              'learning_rate': [1, 0.1, 0.5, 0.01],\n",
        "              }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(adaBoost_t_hypertuned, parameters, scoring=scorer,cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_t, y_train_t)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "adaBoost_t_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "adaBoost_t_hypertuned.fit(X_train_t, y_train_t)"
      ],
      "metadata": {
        "id": "yS0G9sMq2NB0"
      },
      "id": "yS0G9sMq2NB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaBoost_t_hypertuned_score=get_metrics_score(adaBoost_t_hypertuned,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "un7GVUxVvD3w"
      },
      "id": "un7GVUxVvD3w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(adaBoost_t_hypertuned,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "vrFzQmGBvD8Y"
      },
      "id": "vrFzQmGBvD8Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There is no major sign of overfitting and the model is doing a great job of detecting approvals, but the minority class is being ignored."
      ],
      "metadata": {
        "id": "A0TB6r8lwecz"
      },
      "id": "A0TB6r8lwecz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the type of classifier.\n",
        "adaBoost_t_hypertuned_lr = AdaBoostClassifier(base_estimator=LogisticRegression(solver='liblinear',random_state=1),random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              'n_estimators': np.arange(10,120,10),\n",
        "              'learning_rate': [1, 0.1, 0.5, 0.01],\n",
        "              }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(adaBoost_t_hypertuned_lr, parameters, scoring=scorer,cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_log_drop6, y_train_log)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "adaBoost_t_hypertuned_lr = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "adaBoost_t_hypertuned_lr.fit(X_train_log_drop6, y_train_log)"
      ],
      "metadata": {
        "id": "lLG9uhZnvm0M"
      },
      "id": "lLG9uhZnvm0M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaBoost_t_hypertuned_lr_score=get_metrics_score(adaBoost_t_hypertuned_lr,X_train_log_drop6,X_test_log_drop6,y_train_log,y_test_log)"
      ],
      "metadata": {
        "id": "1u-rgZuBwEP_"
      },
      "id": "1u-rgZuBwEP_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(adaBoost_t_hypertuned_lr,y_test_log,X_test_log_drop6)"
      ],
      "metadata": {
        "id": "i4jd59W3wEV7"
      },
      "id": "i4jd59W3wEV7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I see some slight improvements in performance with an increased tendency to ignore the minority class compared to using decison tree as a base estimator.  "
      ],
      "metadata": {
        "id": "Rxjnk6Z_xEZq"
      },
      "id": "Rxjnk6Z_xEZq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient Boost Tuning"
      ],
      "metadata": {
        "id": "diqN8rKCw7Tj"
      },
      "id": "diqN8rKCw7Tj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the type of classifier.\n",
        "grad_Boost_t_hypertuned = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              'n_estimators': np.arange(50,125,25),\n",
        "              'subsample':[0.7,0.8,0.9,1],\n",
        "              'max_features':[0.7,0.8,0.9,1],\n",
        "              'max_depth':[3,5,7,10]\n",
        "              }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(grad_Boost_t_hypertuned, parameters, scoring=scorer,cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_t, y_train_t)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "grad_Boost_t_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "grad_Boost_t_hypertuned.fit(X_train_t, y_train_t)"
      ],
      "metadata": {
        "id": "R7qHqLxy3fG1"
      },
      "id": "R7qHqLxy3fG1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_Boost_t_hypertuned_score=get_metrics_score(grad_Boost_t_hypertuned,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "nrxi9o614BOf"
      },
      "id": "nrxi9o614BOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(grad_Boost_t_hypertuned,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "98yc1o5Q4BTU"
      },
      "id": "98yc1o5Q4BTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decent performance with signs of overfitting. Maybe using XGBoost will help improve the performance."
      ],
      "metadata": {
        "id": "9JKOttRDumMY"
      },
      "id": "9JKOttRDumMY"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train_t.columns\n",
        "importances = grad_Boost_t_hypertuned.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ipTxDNAv8JIY"
      },
      "id": "ipTxDNAv8JIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGradient Boost Tuning"
      ],
      "metadata": {
        "id": "KLIkGcLgw7Yj"
      },
      "id": "KLIkGcLgw7Yj"
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Choose the type of classifier.\n",
        "eXtr_gradBoost_t_hypertuned = XGBClassifier(random_state=1, verbosity = 0)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "              'n_estimators': np.arange(50,125,25),\n",
        "              'subsample':[0.7, 0.8, 0.9, 1],\n",
        "              'gamma':[0, 1, 3, 5],\n",
        "              'colsample_bytree':[0.7, 0.8, 0.9, 1],\n",
        "              'colsample_bylevel':[0.7, 0.8, 0.9, 1]\n",
        "              }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.f1_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(eXtr_gradBoost_t_hypertuned, parameters, scoring=scorer,cv=5,n_jobs=-1)\n",
        "grid_obj = grid_obj.fit(X_train_t, y_train_t)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "eXtr_gradBoost_t_hypertuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "eXtr_gradBoost_t_hypertuned.fit(X_train_t, y_train_t)"
      ],
      "metadata": {
        "id": "MMntFjT67xiA"
      },
      "id": "MMntFjT67xiA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eXtr_gradBoost_t_hypertuned_score=get_metrics_score(eXtr_gradBoost_t_hypertuned,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "CJYqqbw9V995"
      },
      "id": "CJYqqbw9V995",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(eXtr_gradBoost_t_hypertuned,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "YpChB4KqV-Db"
      },
      "id": "YpChB4KqV-Db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It is definitely an improvement over Gradient Boosting. The overfitting problem appears to have been solved and the performance improved! I found this to be actually pretty impressive. Even the f1 score is not showing signs of overfit.\n",
        "- In the final stacking model I should use this as my final estimator. Basically it seems like it has respectable accuracy without sacrificing too much recall."
      ],
      "metadata": {
        "id": "4hx-UZO05y9G"
      },
      "id": "4hx-UZO05y9G"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train_t.columns\n",
        "importances = eXtr_gradBoost_t_hypertuned.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='violet', align='center')\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dN32alRK6_uk"
      },
      "id": "dN32alRK6_uk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Interesting, it has decided to focus more on 'unit_of_wage_hour' than 'education_of_employee'"
      ],
      "metadata": {
        "id": "5buaPLLt7IUJ"
      },
      "id": "5buaPLLt7IUJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking Models"
      ],
      "metadata": {
        "id": "hl0lGbOgPPnH"
      },
      "id": "hl0lGbOgPPnH"
    },
    {
      "cell_type": "code",
      "source": [
        "estimators=[('Decision Tree', dtree_t_hypertuned),('Random Forest', rf_t_hypertuned),\n",
        "           ('Gradient Boosting', grad_Boost_t_hypertuned)]\n",
        "final_estimator=XGBClassifier(random_state=1)"
      ],
      "metadata": {
        "id": "hU2W6nPMPVhG"
      },
      "id": "hU2W6nPMPVhG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the library for stacking model\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stacking_estimator=StackingClassifier(estimators=estimators, final_estimator=final_estimator,cv=5)\n",
        "stacking_estimator.fit(X_train_t,y_train_t)"
      ],
      "metadata": {
        "id": "8RR3O3RCQs7x"
      },
      "id": "8RR3O3RCQs7x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_score=get_metrics_score(stacking_estimator,X_train_t,X_test_t,y_train_t,y_test_t)"
      ],
      "metadata": {
        "id": "87bCqlibRgVc"
      },
      "id": "87bCqlibRgVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_confusion_matrix(stacking_estimator,y_test_t,X_test_t)"
      ],
      "metadata": {
        "id": "0jmEha4-RgbK"
      },
      "id": "0jmEha4-RgbK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Performance very comparible to tuned gradient boost with a major improvement to overfitting, particularly for the f1 score and recall, which are both have proven to be very important scores."
      ],
      "metadata": {
        "id": "2Qdp8Ajxu_6n"
      },
      "id": "2Qdp8Ajxu_6n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obvious-maine"
      },
      "source": [
        "## Model Performance Comparison and Conclusions"
      ],
      "id": "obvious-maine"
    },
    {
      "cell_type": "code",
      "source": [
        "# I print this out seperate because I had to set thresholds and this was my beginner way of solving that problem\n",
        "print(\"Train/Test performance:\")\n",
        "log_reg_model_train_perf,log_reg_model_test_perf"
      ],
      "metadata": {
        "id": "O5xbxCsqGvSU"
      },
      "id": "O5xbxCsqGvSU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "everyday-kinase"
      },
      "outputs": [],
      "source": [
        "# Before tuning performance comparison\n",
        "\n",
        "index=[\"Accuracy-Train\",\"Accuracy-Test\",\"Recall -Train\",\"Recall -Test\",\"Precision-Train\",\"Precision-Test\",\"F1-Train\",\"F1-Test\"]\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [dtree_t_weighted_score, kNN_t_score,bagging_lr_score,bagging_kNN_t_score,\n",
        "    bagging_dtree_t_score,rf_t_score,adaBoost_t_score,gradBoost_t_score,\n",
        "    eXtr_gradBoost_t_score],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "models_train_comp_df.columns = [\n",
        "    \"Decision Tree Weighted\",\n",
        "    \"KNN\",\n",
        "    \"Bagging Logistic Reg\",\n",
        "    \"Bagging KNN\",\n",
        "    \"Bagging Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"AdaBoost\",\n",
        "    \"Gradient Boost\",\n",
        "    \"XGB\"]\n",
        "\n",
        "models_train_comp_df.index=index\n",
        "\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "id": "everyday-kinase"
    },
    {
      "cell_type": "code",
      "source": [
        "# After tuning performance comparison\n",
        "\n",
        "index=[\"Accuracy-Train\",\"Accuracy-Test\",\"Recall -Train\",\"Recall -Test\",\"Precision-Train\",\"Precision-Test\"\"Precision-Test\",\"F1-Train\",\"F1-Test\"]\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [dtree_t_hypertuned_score, kNN_t_hypertuned_score,bagging_lr_hypertuned_score,bagging_dtree_t_hypertuned_score,\n",
        "    rf_t_hypertuned_score,adaBoost_t_hypertuned_score,grad_Boost_t_hypertuned_score,eXtr_gradBoost_t_hypertuned_score,\n",
        "    stacked_score],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "models_train_comp_df.columns = [\n",
        "    \"Decision Tree Tuned\",\n",
        "    \"KNN Tuned\",\n",
        "    \"Baggin Logistic Reg Tuned\",\n",
        "    \"Bagging Decision Tree Tuned\",\n",
        "    \"Random Forest Tuned\",\n",
        "    \"AdaBoost Tuned\",\n",
        "    \"Gradient Boost Tuned\",\n",
        "    \"XGB Tuned\",\n",
        "    \"Stacking Tuned\"]\n",
        "models_train_comp_df.index=index\n",
        "\n",
        "print(\"Tuned performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "metadata": {
        "id": "vhvkzkWuvrrZ"
      },
      "id": "vhvkzkWuvrrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nasty-retailer"
      },
      "source": [
        "## Actionable Insights and Recommendations"
      ],
      "id": "nasty-retailer"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights:\n",
        "  - Specifically, `education_of_employee`, `has_job_experience`, `continent_Europe`, and `unit_of_wage` were indicated as important features by the analysis.\n",
        "      - This was further backed up by the EDA. For example:\n",
        "        - \"Europe\" was the most likely to be \"Certified\".\n",
        "        - \"Hourly\" was the most likely to be \"Denied\" and so on.\n",
        "  - The stacking model appears to provide the best performance and offers the benefit of enhanced generalization. However the models were not perfect and some errors will be made in shortlisting candidates.<br></br>\n",
        "\n",
        "Recommendations:\n",
        "- I recommend using the stacked model to shortlist applicants, but then utilize a secondary screen to catch mistakes made in shortlisting applicants.\n",
        "  - The secondary screen can be a streamlined version of the original visa application process.\n",
        "    - Perhaps it can take advantage of the primary screen to speed up the process. For instance, a shortlisted candidate that has a Doctorate with job experience from Europe and with a yearly salary could be expedited.\n",
        "- Of all the features, look out for yearly salaried employees, highly educated individuals, and especially if they have job experience because they are good candidates for shortlisting without adding alot of error.\n"
      ],
      "metadata": {
        "id": "sgQ6cKWJdFBX"
      },
      "id": "sgQ6cKWJdFBX"
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/DSBA_Project_ET_EasyVisa_Fullcode.ipynb"
      ],
      "metadata": {
        "id": "vpI97W_g9fcU"
      },
      "id": "vpI97W_g9fcU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "akEfhT4E9glT"
      },
      "id": "akEfhT4E9glT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "df33ac2b",
        "8d6ae8be",
        "365e07e5",
        "XvOQKILD7xbV",
        "cTUtIcbYwDMg",
        "NK6uX5mDEQGI",
        "G7TDRPfKEcWH",
        "pIjhWAFsv-JO",
        "u9QNi7cuMwRv",
        "pjefVaE2wIP7",
        "rJ4uzckhwLgZ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}